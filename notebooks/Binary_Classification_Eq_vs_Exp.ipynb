{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e726ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns \n",
    "\n",
    "# for converting the text file containing the quarry locations into csv file\n",
    "import csv\n",
    "\n",
    "# for computing the geographical distance between two points \n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, auc, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import obspy\n",
    "from obspy.geodetics.base import gps2dist_azimuth, gps2dist_azimuth\n",
    "from obspy.clients.fdsn import Client\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "from joblib import dump, load\n",
    "from obspy.signal.filter import envelope\n",
    "import tsfel\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../feature_extraction_scripts/physical_feature_extraction_scripts')\n",
    "import seis_feature\n",
    "#from seis_feature import compute_physical_features\n",
    "from tsfel import time_series_features_extractor, get_features_by_domain\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../common_scripts')\n",
    "\n",
    "from common_processing_functions import apply_cosine_taper\n",
    "from common_processing_functions import butterworth_filter\n",
    "\n",
    "import pickle\n",
    "from zenodo_get import zenodo_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c6fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45c18cb8",
   "metadata": {},
   "source": [
    "## Some custom helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some helpful functions in plotting \n",
    "def interquartile(df):\n",
    "\n",
    "    # Set the lower and upper quantile thresholds (25% and 75%)\n",
    "    lower_quantile = 0.01\n",
    "    upper_quantile = 0.99\n",
    "\n",
    "    # Filter the DataFrame based on the quantile range for all columns\n",
    "    filtered_df = df[\n",
    "        (df >= df.quantile(lower_quantile)) &\n",
    "        (df <= df.quantile(upper_quantile))\n",
    "    ]\n",
    "\n",
    "    # Drop rows with any NaN values (if needed)\n",
    "    #filtered_df = filtered_df.dropna(axis = 1)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "trace_cm_phy_tsf_man = []\n",
    "trace_report_phy_tsf_man = []\n",
    "def plot_confusion_matrix(cf = trace_cm_phy_tsf_man):\n",
    "\n",
    "    \n",
    "    annot_kws = {\"fontsize\": 15}\n",
    "    labels = ['Precision', 'Recall', 'F1-Score']\n",
    "    class_labels = ['Earthquake', 'Explosion']\n",
    "    plt.figure(figsize = [8,6])\n",
    "\n",
    "    # Set annotation font size within each block\n",
    "    annot_kws = {\"fontsize\": 15}\n",
    "    ax = sns.heatmap(cf, annot=True, cmap='Blues', fmt='d', xticklabels = class_labels, yticklabels = class_labels, annot_kws=annot_kws)\n",
    "\n",
    "\n",
    "    # Set tick label font size\n",
    "    ax.set_xticklabels(class_labels, fontsize=15)\n",
    "    ax.set_yticklabels(class_labels, fontsize=15)\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted', fontsize = 15)\n",
    "    plt.ylabel('Actual', fontsize = 15)\n",
    "    plt.title('Total samples: '+str(len(y_pred)), fontsize = 20)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "annot_kws = {\"fontsize\": 15}\n",
    "def plot_classification_report(cr = trace_report_phy_tsf_man): \n",
    "\n",
    "    labels = ['Precision', 'Recall', 'F1-Score']\n",
    "    class_labels = ['Earthquake', 'Explosion']\n",
    "    # Set a pleasing style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    \n",
    "    # Create a figure and axes for the heatmap\n",
    "    plt.figure(figsize = [8,6])\n",
    "    ax = sns.heatmap(pd.DataFrame(cr).iloc[:3, :2], annot=True, cmap='Blues', yticklabels = labels, xticklabels=class_labels, vmin=0.8, vmax=1, annot_kws=annot_kws)\n",
    "\n",
    "    # Set labels and title\n",
    "    # Set tick label font size\n",
    "    ax.set_xticklabels(class_labels, fontsize=15)\n",
    "    ax.set_yticklabels(labels, fontsize=15)\n",
    "\n",
    "    ax.set_xlabel('Metrics', fontsize=15)\n",
    "    ax.set_ylabel('Classes', fontsize=15)\n",
    "    ax.set_title('Classification Report', fontsize=18)\n",
    "\n",
    "    # Create a colorbar\n",
    "    #cbar = ax.collections[0].colorbar\n",
    "    #cbar.set_ticks([0.5, 1])  # Set custom tick locations\n",
    "    #cbar.set_ticklabels(['0', '0.5', '1'])  # Set custom tick labels\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Calculate the difference in latitude and longitude\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Calculate the distance using the Haversine formula\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3ca3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa6b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da6f0b46",
   "metadata": {},
   "source": [
    "## Downloading features from a repository on Zenodo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58faa8e",
   "metadata": {},
   "source": [
    "### Donwloading tsfel features from a 150s window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading tsfel features\n",
    "\n",
    "\n",
    "## Downloading the physical features \n",
    "## This is doi for physical features (for both 50s and 150s) - 10.5281/zenodo.10689585\n",
    "## This is doi for tsfel features of 50s window length - 10.5281/zenodo.10689673\n",
    "## This is doi for tsfel features of 150s window length - 10.5281/zenodo.10689824\n",
    "\n",
    "doi = '10.5281/zenodo.10689824'  #Downloading tsfel features for 150s length\n",
    "# Download files from the Zenodo record\n",
    "files = zenodo_get([doi])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5dee63",
   "metadata": {},
   "source": [
    "### Downloading physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e19cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading the updated physical features \n",
    "\n",
    "doi = '10.5281/zenodo.11193585'\n",
    "#Downloading physical\n",
    "# Download files from the Zenodo record\n",
    "files = zenodo_get([doi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70afccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9824b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e79dfb0",
   "metadata": {},
   "source": [
    "## Loading tsfel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These waveforms are filtered between 1-10 Hz\n",
    "# extracting features of surface events, thunder and sonic booms\n",
    "features_exotic_tsfel = pd.read_csv('tsfel_features_exotic_50_100.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# features of noise\n",
    "features_noise_tsfel = pd.read_csv('tsfel_features_noise_50_100.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# features of explosion\n",
    "features_explosion_tsfel = pd.read_csv('tsfel_features_comcat_z_explosion_50_100.csv')\n",
    "\n",
    "\n",
    "\n",
    "# features of earthquake (had to extract it in three parts because of memory constraints)\n",
    "features_eq1 = pd.read_csv('tsfel_features_comcat_z_earthquake_50_100_part1.csv')\n",
    "features_eq2 = pd.read_csv('tsfel_features_comcat_z_earthquake_50_100_part2.csv')\n",
    "features_eq3 = pd.read_csv('tsfel_features_comcat_z_earthquake_50_100_part3.csv')\n",
    "\n",
    "\n",
    "\n",
    "# features of earthquakes\n",
    "features_earthquake_tsfel = pd.concat([features_eq1, features_eq2, features_eq3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141acf38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a979dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8863194",
   "metadata": {},
   "source": [
    "## Loading physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features of surface events, thunder and sonic booms\n",
    "features_exotic_physical = pd.read_csv('new_physical_features_exotic_z_50_100.csv')\n",
    "\n",
    "features_surface_physical = features_exotic_physical[features_exotic_physical['source'] == 'surface']\n",
    "features_sonic_physical = features_exotic_physical[features_exotic_physical['source'] == 'sonic']\n",
    "features_thunder_physical = features_exotic_physical[features_exotic_physical['source'] == 'thunder']\n",
    "\n",
    "\n",
    "\n",
    "# features of noise\n",
    "features_noise_physical = pd.read_csv('new_physical_features_noise_z_50_100.csv')\n",
    "\n",
    "\n",
    "# features of explosion\n",
    "features_explosion_physical = pd.read_csv('new_physical_features_comcat_z_explosion_50_100.csv')\n",
    "\n",
    "# features of earthquakes\n",
    "features_eq1 = pd.read_csv('new_physical_features_earthquake_z_part1_50_100.csv')\n",
    "features_eq2 = pd.read_csv('new_physical_features_earthquake_z_part2_50_100.csv')\n",
    "features_eq3 = pd.read_csv('new_physical_features_earthquake_z_part3_50_100.csv')\n",
    "features_eq4 = pd.read_csv('new_physical_features_earthquake_z_part4_50_100.csv')\n",
    "\n",
    "features_earthquake_physical = pd.concat([features_eq1, features_eq2, features_eq3, features_eq4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ede3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94518e1f",
   "metadata": {},
   "source": [
    "## Removing the large files from the directory\n",
    "\n",
    "The features files we downloaded are very large in size. This will create problems when uploading to github repository so once we store the features in variable we will remove these files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "# List the files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Iterate over the files and delete CSV files\n",
    "for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c9d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c96b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6a83a71",
   "metadata": {},
   "source": [
    "## Merging tsfel and physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each event is identified by a serial number. The serial number is the iteration number when we iterated through \n",
    "# the catalog, since tsfel and physical features were extracted separately, we have to merge them together. \n",
    "features_noise = pd.merge(features_noise_physical, features_noise_tsfel, on = 'serial_no')\n",
    "features_earthquake = pd.merge(features_earthquake_physical, features_earthquake_tsfel, on = 'serial_no')\n",
    "features_explosion = pd.merge(features_explosion_physical, features_explosion_tsfel, on = 'serial_no')\n",
    "\n",
    "\n",
    "\n",
    "# The exotic events further contains three classes - (i) surface, (ii) sonic and (iii) thunder\n",
    "# since sonic and thunder contain very few events, we are going to exclude them fron our analysis from now on. \n",
    "features_surface_tsfel = features_exotic_tsfel[features_exotic_tsfel['source'] == 'surface']\n",
    "features_sonic_tsfel = features_exotic_tsfel[features_exotic_tsfel['source'] == 'sonic']\n",
    "features_thunder_tsfel = features_exotic_tsfel[features_exotic_tsfel['source'] == 'thunder']\n",
    "\n",
    "features_surface_physical = features_exotic_physical[features_exotic_physical['source'] == 'surface']\n",
    "features_sonic_physical = features_exotic_physical[features_exotic_physical['source'] == 'sonic']\n",
    "features_thunder_physical = features_exotic_physical[features_exotic_physical['source'] == 'thunder']\n",
    "\n",
    "\n",
    "features_surface = pd.merge(features_surface_physical, features_surface_tsfel, on = 'serial_no')\n",
    "features_sonic = pd.merge(features_sonic_physical, features_sonic_tsfel, on = 'serial_no')\n",
    "features_thunder = pd.merge(features_thunder_physical, features_thunder_tsfel, on = 'serial_no')\n",
    "\n",
    "\n",
    "## Concatenating all the features from all classes. \n",
    "features_all = pd.concat([features_surface, features_sonic, features_thunder, features_noise, features_explosion, features_earthquake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135a0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019863f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b393b67",
   "metadata": {},
   "source": [
    "### Removing correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeba14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nos = features_all['serial_no'].values\n",
    "features_all = features_all.drop(['Unnamed: 0_x','Unnamed: 0_y', 'source_x', 'serial_no'], axis = 1, errors = 'ignore')\n",
    "features_all.rename(columns={'source_y': 'source'}, inplace=True)\n",
    "corr_features = tsfel.correlated_features(features_all.iloc[:, 1:453])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c03ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all.drop(corr_features, axis=1, inplace=True)\n",
    "features_all['serial_no'] = serial_nos\n",
    "print(f'So we have {features_all.shape[0]} events and each event have {features_all.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5123a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2fe24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0424adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841008b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e65560c",
   "metadata": {},
   "source": [
    "## Preprocessing the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5efe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns that contain NaNs\n",
    "features_all = features_all.dropna(axis = 1)\n",
    "\n",
    "# dropping the rows that contains NaNs\n",
    "features_all = features_all.dropna()\n",
    "\n",
    "\n",
    "## dropping all the rows containing infinity values\n",
    "features_all = features_all.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "\n",
    "## dropping sonic boom and thunder events\n",
    "features_all = features_all[features_all['source'] != 'sonic']\n",
    "features_all = features_all[features_all['source'] != 'thunder']\n",
    "\n",
    "print(f'So we have {features_all.shape[0]} events and each event have {features_all.shape[1]} features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db8338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3698c3f7",
   "metadata": {},
   "source": [
    "## Dropping the columns that contain same values for all the events, as these features do not really contribute in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d71530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in each column\n",
    "unique_counts = features_all.nunique()\n",
    "\n",
    "# Identify columns with only one unique value (same value for all rows)\n",
    "single_value_columns = unique_counts[unique_counts == 1].index\n",
    "\n",
    "\n",
    "# Drop columns with the same value for all rows\n",
    "features_all = features_all.drop(columns=single_value_columns)\n",
    "\n",
    "print(f'So we have {features_all.shape[0]} events and each event have {features_all.shape[1]} features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23896847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a8f713e",
   "metadata": {},
   "source": [
    "## Removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features_all.drop(['serial_no', 'source'], axis = 1)\n",
    "# Calculate Z-scores for each feature\n",
    "z_scores = np.abs(stats.zscore(df))\n",
    "\n",
    "\n",
    "# Define a threshold for Z-score beyond which data points are considered outliers\n",
    "threshold = 4\n",
    "\n",
    "# Filter out rows with any Z-score greater than the threshold\n",
    "# Temporarily removing this \n",
    "outliers_removed_df =   features_all[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "print(len(features_all))\n",
    "print(len(outliers_removed_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e9dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0caa675",
   "metadata": {},
   "source": [
    "## Standardizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6355cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the global variables X and y\n",
    "X = outliers_removed_df.drop(['serial_no','source'], axis = 1)\n",
    "y = outliers_removed_df['source']\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "# Apply standard scaling to the DataFrame\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Access the mean and standard deviation for each feature\n",
    "means = scaler.mean_\n",
    "std_devs = scaler.scale_\n",
    "\n",
    "# Create a DataFrame to display the means and standard deviations\n",
    "scaler_params = pd.DataFrame({'Feature': X.columns, 'Mean': means, 'Std Dev': std_devs})\n",
    "print(scaler_params)\n",
    "\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=X.columns)\n",
    "\n",
    "\n",
    "#We are not standardizing at this stage. We will rather wait when the outlier are removed, then we will\n",
    "## standardize and save the standard scaler parameters. \n",
    "X_scaled = X\n",
    "\n",
    "X_scaled['serial_no'] = outliers_removed_df['serial_no'].values\n",
    "X_scaled['source'] = outliers_removed_df['source'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485648f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3fff1a7",
   "metadata": {},
   "source": [
    "## Merging the Metadata information with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95778dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the stored data\n",
    "comcat_file_name = h5py.File(\"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\",'r')\n",
    "exotic_file_name = h5py.File(\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\",'r')\n",
    "noise_file_name = h5py.File(\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\",'r')\n",
    "\n",
    "\n",
    "# extracting the catalog\n",
    "comcat_file_csv = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "exotic_file_csv = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "noise_file_csv = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# extracting the metadata corresponding to individual events\n",
    "cat_exp = comcat_file_csv[comcat_file_csv['source_type'] == 'explosion']\n",
    "cat_eq = comcat_file_csv[comcat_file_csv['source_type'] == 'earthquake']\n",
    "cat_no = noise_file_csv\n",
    "cat_su = exotic_file_csv[exotic_file_csv['source_type'] == 'surface event']\n",
    "\n",
    "\n",
    "\n",
    "# extracting the index \n",
    "ind_exp = X_scaled[X_scaled['source'] == 'explosion']['serial_no'].values\n",
    "ind_eq = X_scaled[X_scaled['source'] == 'earthquake']['serial_no'].values\n",
    "ind_no = X_scaled[X_scaled['source'] == 'noise']['serial_no'].values\n",
    "ind_su = X_scaled[X_scaled['source'] == 'surface']['serial_no'].values\n",
    "\n",
    "\n",
    "df_exp = X_scaled[X_scaled['source'] == 'explosion']\n",
    "exp_df = cat_exp.iloc[ind_exp]\n",
    "exp_df['serial_no'] = ind_exp\n",
    "\n",
    "\n",
    "df_eq = X_scaled[X_scaled['source'] == 'earthquake']\n",
    "eq_df = cat_eq.iloc[ind_eq]\n",
    "eq_df['serial_no'] = ind_eq\n",
    "\n",
    "\n",
    "\n",
    "df_no = X_scaled[X_scaled['source'] == 'noise']\n",
    "no_df = cat_no.iloc[ind_no]\n",
    "no_df['serial_no'] = ind_no\n",
    "\n",
    "\n",
    "\n",
    "df_su = X_scaled[X_scaled['source'] == 'surface']\n",
    "su_df = cat_su.iloc[ind_su]\n",
    "su_df['serial_no'] = ind_su\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_exp = pd.merge(df_exp,exp_df, on = 'serial_no')\n",
    "new_eq = pd.merge(df_eq,eq_df, on = 'serial_no')\n",
    "new_su = pd.merge(df_su,su_df, on = 'serial_no')\n",
    "new_no = pd.merge(df_no,no_df, on = 'serial_no')\n",
    "new_no['event_id'] = np.array(['noise'+str(i) for i in np.arange(len(new_no))])\n",
    "\n",
    "\n",
    "\n",
    "X_final = pd.concat([new_exp, new_eq, new_su, new_no])\n",
    "y = ['explosion']*len(new_exp)+['earthquake']*len(new_eq)+['surface']*len(new_su)+['noise']*len(new_no)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131f771",
   "metadata": {},
   "source": [
    "### Adding Hour of the Day, Days of Week and Month of Year as additional features as they have shown to significantly increase the performance of the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_exp contains the features and the corresponding metadata information. \n",
    "datetimes = X_final['trace_start_time'].values\n",
    "\n",
    "hour_of_day = []\n",
    "days_of_week = []\n",
    "month_of_year = []\n",
    "for dt_str in tqdm(datetimes):\n",
    "        \n",
    "    # Parse the datetime string\n",
    "        dt = datetime.strptime(dt_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        hod = dt.hour - 8.  # converting to local time. \n",
    "        moy = dt.month\n",
    "        \n",
    "        \n",
    "        days_of_week.append(dt.weekday())\n",
    "        hour_of_day.append(hod)\n",
    "        month_of_year.append(moy)\n",
    "        \n",
    "X_final['hour_of_day'] = hour_of_day\n",
    "X_final['day_of_week'] = days_of_week\n",
    "X_final['month_of_year'] = month_of_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db215819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080ddad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dfc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4b8252e",
   "metadata": {},
   "source": [
    "## Standardizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b76d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = X_final.iloc[:,0:int(np.where(X_final.columns == 'serial_no')[0])]\n",
    "#temp_X = temp_X.assign(hod=X_final['hour_of_day'].values, dow=X_final['day_of_week'].values, moy=X_final['month_of_year'].values)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply standard scaling to the DataFrame\n",
    "scaled_features = scaler.fit_transform(temp_X)\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "temp_X = pd.DataFrame(scaled_features, columns= temp_X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59d7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f34b36",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of the random forest model for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0704f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random undersampling using imbalanced-learn library\n",
    "rus = RandomUnderSampler(sampling_strategy={'earthquake':3000, 'explosion':3000, 'surface':0, 'noise':0})\n",
    "X_resampled, y_resampled = rus.fit_resample(temp_X, y)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the text labels and transform them to numeric labels\n",
    "y_num = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_num, test_size=0.2, stratify = y_num)\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid for randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize the RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, param_distributions=param_dist, n_iter=50, scoring='f1_macro', cv=10, verbose=0, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform randomized grid search cross-validation\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc92ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c932d2e4",
   "metadata": {},
   "source": [
    "## Performance with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdca517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance with best model\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a6f91",
   "metadata": {},
   "source": [
    "## Classification report with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d1a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "plot_classification_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab46676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "490b8b0a",
   "metadata": {},
   "source": [
    "## Computing the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ba2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 10\n",
    "f_imp = []\n",
    "\n",
    "for i in tqdm(range(num_iter)):\n",
    "\n",
    "    # Apply random undersampling using imbalanced-learn library\n",
    "    rus = RandomUnderSampler(sampling_strategy={'earthquake':3000, 'explosion':3000, 'surface':0, 'noise':0})\n",
    "    X_resampled, y_resampled = rus.fit_resample(temp_X, y)\n",
    "\n",
    "    # Initialize the LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit the LabelEncoder on the text labels and transform them to numeric labels\n",
    "    y_num = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_num, test_size=0.2, stratify = y_num)\n",
    "\n",
    "    # Perform randomized grid search cross-validation\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    f_imp.append(best_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867e2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee980701",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_imp = np.mean(f_imp, axis = 0)\n",
    "features = X_train.columns.values\n",
    "feature_colors = ['darkred']*49+['darkblue']*(len(features)-49)  # red for physical and blue for tsfel features. \n",
    "\n",
    "## Removing the 0 from the starting of each feature name\n",
    "## the second argument in split specifies the maxsplit, the amount of times the split will be performed. \n",
    "\n",
    "feature_names = np.hstack([features[0:49],[features[49:][i].split('_', 1)[1] for i in range(len(features[49:]))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ff823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the array to disk\n",
    "np.save('../results/final_imp_binary.npy', final_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace5dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20e052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db107226",
   "metadata": {},
   "source": [
    "## Plotting the most important features for binary discrimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af99090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample feature importances and feature labels\n",
    "feature_importances = final_imp\n",
    "feature_labels = feature_names\n",
    "\n",
    "# Sort feature importances and feature labels together\n",
    "sorted_indices = sorted(range(len(feature_importances)), key=lambda k: feature_importances[k], reverse=True)\n",
    "sorted_feature_importances = [feature_importances[i] for i in sorted_indices]\n",
    "sorted_feature_labels = [feature_labels[i] for i in sorted_indices]\n",
    "colors = [feature_colors[i] for i in sorted_indices]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 10))\n",
    "bars = plt.barh(sorted_feature_labels[0:50], sorted_feature_importances[0:50])\n",
    "\n",
    "# Color bars to match yticklabels\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "# Color yticklabels\n",
    "for label, color in zip(plt.gca().get_yticklabels(), colors):\n",
    "    label.set_color(color)\n",
    "\n",
    "# Create legend handles and labels\n",
    "legend_handles = [plt.Rectangle((0,0),1,1, color='darkblue', ec='black'), plt.Rectangle((0,0),1,1, color='darkred', ec='black')]\n",
    "legend_labels = ['Tsfel Features', 'Physical Features']\n",
    "\n",
    "plt.legend(legend_handles, legend_labels, title='Features', fontsize='large', title_fontsize='large', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "plt.xlabel('Feature Importance', fontsize=15)\n",
    "plt.ylabel('Feature', fontsize=15)\n",
    "plt.title('Top 50 Feature Importances', fontsize=15)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to display highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1731d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113be25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1bfc8fa",
   "metadata": {},
   "source": [
    "## Plotting the distribution of Imp features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_df = features_noise_physical\n",
    "eq_df = features_earthquake_physical\n",
    "su_df = features_exotic_physical[features_exotic_physical['source'] == 'surface']\n",
    "exp_df = features_explosion_physical\n",
    "\n",
    "\n",
    "filtered_no = interquartile(no_df)\n",
    "filtered_su = interquartile(su_df)\n",
    "filtered_eq = interquartile(eq_df)\n",
    "filtered_exp = interquartile(exp_df)\n",
    "\n",
    "\n",
    "plt.style.use('default')\n",
    "# Set rc parameters for font size\n",
    "plt.rcParams['xtick.labelsize'] = 16  # Font size for xtick labels\n",
    "plt.rcParams['ytick.labelsize'] = 16  # Font size for ytick labels\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = [15,10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'Kurto_0.1_1' ## The time taken to reach the maximum amplitude of Envelope\n",
    "l1 = 0\n",
    "l2 = 40\n",
    "#all_df = features_all\n",
    "#data_range = all_df[feature].values.ptp()\n",
    "num_bins = 100 #int(np.ceil(data_range / 0.5))\n",
    "#ax[0,0].hist(x= filtered_no[feature].values, bins= num_bins, density = True,\n",
    "#          color =  '#d62728', alpha = 0.5,label = 'Noise')\n",
    "#ax[0,0].hist(x= filtered_su[feature].values, bins= num_bins,density = True,    \n",
    "#          color = '#2ca02c', alpha = 0.5, label = 'Surface')\n",
    "ax[0,0].hist(x= filtered_eq[feature].values, bins= num_bins,density = True,  \n",
    "               color = '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "ax[0,0].hist(x= filtered_exp[feature].values, bins=num_bins,density = True,  \n",
    " color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "ax[0,0].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "#ax[0,0].set_yscale('log')\n",
    "ax[0,0].set_xlim(l1,l2)\n",
    "ax[0,0].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'DistMaxMeanFreqDTF' ## The time taken to reach the maximum amplitude of Envelope\n",
    "l1 = 0\n",
    "l2 = 0.01\n",
    "#all_df = features_all\n",
    "#data_range = all_df[feature].values.ptp()\n",
    "num_bins = 200 #int(np.ceil(data_range / 0.5))\n",
    "#ax[0,1].hist(x= filtered_no[feature].values, bins= num_bins, density = True,\n",
    "#          color =  '#d62728', alpha = 0.5,label = 'Noise')\n",
    "#ax[0,1].hist(x= filtered_su[feature].values, bins= num_bins,density = True,    \n",
    "#          color = '#2ca02c', alpha = 0.5, label = 'Surface')\n",
    "ax[0,1].hist(x= filtered_eq[feature].values, bins= num_bins,density = True,  \n",
    "               color = '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "ax[0,1].hist(x= filtered_exp[feature].values, bins= num_bins,density = True,  \n",
    "   color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "ax[0,1].legend()\n",
    "#ax[0,1].set_xscale('log')\n",
    "#ax[0,1].set_yscale('log')\n",
    "ax[0,1].set_xlim(l1,l2)\n",
    "ax[0,1].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'VarFFT' ## The Kurtosis of envelope\n",
    "l1 = 0\n",
    "l2 = 10\n",
    "\n",
    "#all_df = features_all\n",
    "#data_range = all_df[feature].values.ptp()\n",
    "num_bins = 50 #int(np.ceil(data_range / 0.5))\n",
    "#ax[1,0].hist(x= filtered_no[feature].values, bins= 400, density = True,\n",
    "#          color =  '#d62728', alpha = 0.5,label = 'Noise')\n",
    "#ax[1,0].hist(x= filtered_su[feature].values, bins= 50,density = True,    \n",
    "#          color = '#2ca02c', alpha = 0.5, label = 'Surface')\n",
    "ax[1,0].hist(x= filtered_eq[feature].values, bins= num_bins,density = True,  \n",
    "               color = '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "ax[1,0].hist(x= filtered_exp[feature].values, bins= num_bins,density = True,  \n",
    "   color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "ax[1,0].legend()\n",
    "#ax[1,0].set_yscale('log')\n",
    "#ax[1,0].set_xscale('log')\n",
    "#ax[1,0].set_xlim(l1,l2)\n",
    "ax[1,0].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'FCentroid' ## ratio of Max over median of envelope\n",
    "l1 = 0\n",
    "l2 = 0.01\n",
    "\n",
    "#all_df = features_all\n",
    "#data_range = all_df[feature].values.ptp()\n",
    "num_bins = 50 #int(np.ceil(data_range / 0.5))\n",
    "#ax[1,1].hist(x= filtered_no[feature].values, bins= 500, density = True,\n",
    "#          color =  '#d62728', alpha = 0.5,label = 'Noise')\n",
    "#ax[1,1].hist(x= filtered_su[feature].values, bins= num_bins,density = True,    \n",
    "#          color = '#2ca02c', alpha = 0.5, label = 'Surface')\n",
    "ax[1,1].hist(x= filtered_eq[feature].values, bins= 50,density = True,  \n",
    "               color = '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "ax[1,1].hist(x= filtered_exp[feature].values, bins=num_bins,density = True,  \n",
    "   color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "ax[1,1].legend(loc = 'upper right')\n",
    "ax[1,1].set_xscale('log')\n",
    "#ax[1,1].set_yscale('log')\n",
    "#ax[1,1].set_xlim(l1,l2)\n",
    "ax[1,1].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "feature = 'KurtoEnv' ## ratio of Max over median of envelope\n",
    "\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.01\n",
    "\n",
    "#all_df = features_all\n",
    "#data_range = all_df[feature].values.ptp()\n",
    "num_bins = 50 #int(np.ceil(data_range / 0.5))\n",
    "#ax[1,1].hist(x= filtered_no[feature].values, bins= 500, density = True,\n",
    "#          color =  '#d62728', alpha = 0.5,label = 'Noise')\n",
    "#ax[1,1].hist(x= filtered_su[feature].values, bins= num_bins,density = True,    \n",
    "#          color = '#2ca02c', alpha = 0.5, label = 'Surface')\n",
    "ax[2,0].hist(x= filtered_eq[feature].values, bins= 50,density = True,  \n",
    "               color = '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "ax[2,0].hist(x= filtered_exp[feature].values, bins=num_bins,density = True,  \n",
    "   color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "\n",
    "ax[2,0].legend(loc = 'upper right')\n",
    "ax[2,0].set_xscale('log')\n",
    "#ax[1,1].set_yscale('log')\n",
    "#ax[1,1].set_xlim(l1,l2)\n",
    "\n",
    "\n",
    "ax[2,0].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'DistQ3Q1DFT' ## ratio of Max over median of envelope\n",
    "\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.01\n",
    "\n",
    "#all_df = features_all\n",
    "#data_range = all_df[feature].values.ptp()\n",
    "num_bins = 50 #int(np.ceil(data_range / 0.5))\n",
    "#ax[1,1].hist(x= filtered_no[feature].values, bins= 500, density = True,\n",
    "#          color =  '#d62728', alpha = 0.5,label = 'Noise')\n",
    "#ax[1,1].hist(x= filtered_su[feature].values, bins= num_bins,density = True,    \n",
    "#          color = '#2ca02c', alpha = 0.5, label = 'Surface')\n",
    "ax[2,1].hist(x= filtered_eq[feature].values, bins= 50,density = True,  \n",
    "               color = '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "ax[2,1].hist(x= filtered_exp[feature].values, bins=num_bins,density = True,  \n",
    "   color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "\n",
    "ax[2,1].legend(loc = 'upper right')\n",
    "#ax[2,1].set_xscale('log')\n",
    "#ax[1,1].set_yscale('log')\n",
    "#ax[1,1].set_xlim(l1,l2)\n",
    "\n",
    "\n",
    "ax[2,1].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.supylabel('Density', fontsize = 20)\n",
    "fig.supxlabel('Value of Features', fontsize = 20)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b649e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79fe0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7d4ca64",
   "metadata": {},
   "source": [
    "### Plotting the most important tsfel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89358af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_df = features_noise_tsfel\n",
    "eq_df = features_earthquake_tsfel\n",
    "su_df = features_exotic_tsfel[features_exotic_tsfel['source'] == 'surface']\n",
    "exp_df = features_explosion_tsfel\n",
    "\n",
    "filtered_no = interquartile(no_df)\n",
    "filtered_su = interquartile(su_df)\n",
    "filtered_eq = interquartile(eq_df)\n",
    "filtered_exp = interquartile(exp_df)\n",
    "\n",
    "\n",
    "# Define a function for histogram plotting\n",
    "def plot_histogram(ax, feature_data, label, color, num_bins, title):\n",
    "    ax.hist(x=feature_data, bins=num_bins, density=True, color=color, alpha=0.5, label=label)\n",
    "    ax.legend()\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "# Define common parameters\n",
    "num_bins = 50\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=[15, 10])\n",
    "\n",
    "# Define features and labels\n",
    "features = {\n",
    "    '0_Max power spectrum': [filtered_eq, filtered_exp],\n",
    "    '0_Spectral entropy': [filtered_eq, filtered_exp],\n",
    "    '0_Histogram_2': [filtered_eq, filtered_exp],\n",
    "    '0_FFT mean coefficient_9': [filtered_eq, filtered_exp]\n",
    "}\n",
    "labels = ['Earthquake', 'Explosion']\n",
    "colors = ['#1f77b4', '#9467bd']\n",
    "\n",
    "# Iterate over features and plot histograms\n",
    "for idx, (feature, (filtered_eq, filtered_exp)) in enumerate(features.items()):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    plot_histogram(ax[row, col], filtered_eq[feature].values, labels[0], colors[0], num_bins, feature.split('_')[1])\n",
    "    plot_histogram(ax[row, col], filtered_exp[feature].values, labels[1], colors[1], num_bins, feature.split('_')[1])\n",
    "\n",
    "fig.supylabel('Density', fontsize=20)\n",
    "fig.supxlabel('Value of Features', fontsize=20)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00978e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd16fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c2563a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c396a09",
   "metadata": {},
   "source": [
    "## Selecting 5000 random events per class for training and testing on all the remaining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting metadata information for each kind of source along with features\n",
    "a_eq = X_final[X_final['source_type_pnsn_label'] == 'eq']\n",
    "## removing the ambiguous events\n",
    "a_eq = a_eq[a_eq['source_type'] != 'explosion']\n",
    "\n",
    "a_px = X_final[X_final['source_type_pnsn_label'] == 'px']\n",
    "# removing the ambiguous events. \n",
    "a_px = a_px[a_px['source_type'] != 'earthquake']\n",
    "\n",
    "a_su = X_final[X_final['source_type'] == 'surface event']\n",
    "a_no = X_final[X_final['source_type'] == 'noise']\n",
    "\n",
    "\n",
    "\n",
    "## extracting the event ids corresponding to each catalog\n",
    "eq_ids = np.unique(a_eq['event_id'].values)\n",
    "su_ids = np.unique(a_su['event_id'].values)\n",
    "no_ids = np.unique(a_no['event_id'].values)\n",
    "px_ids = np.unique(a_px['event_id'].values)\n",
    "\n",
    "\n",
    "## defining the events for training and testing in 70:30 ratio\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Modifying this section a bit, for a fair comparison to deep neural network. \n",
    "\n",
    "\n",
    "train_eq = eq_ids[0:int(0.7*len(eq_ids))]\n",
    "train_px = px_ids[0:int(0.7*len(px_ids))]\n",
    "train_su = su_ids[0:int(0.7*len(su_ids))]\n",
    "train_no = no_ids[0:int(0.7*len(no_ids))]\n",
    "\n",
    "\n",
    "test_eq = eq_ids[int(0.7*len(eq_ids)):len(eq_ids)]\n",
    "test_px = px_ids[int(0.7*len(px_ids)):len(px_ids)]\n",
    "test_su = su_ids[int(0.7*len(su_ids)):len(su_ids)]\n",
    "test_no = no_ids[int(0.7*len(no_ids)):len(no_ids)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "## randomizing along the time. \n",
    "r1 = np.random.randint(0, len(eq_ids), 5000)\n",
    "train_eq = eq_ids[r1]\n",
    "\n",
    "## randomizing along the time. \n",
    "r2 = np.random.randint(0, len(px_ids), 5000)\n",
    "train_px = px_ids[r2]\n",
    "\n",
    "## randomizing along the time. \n",
    "r3 = np.random.randint(0, len(su_ids), 5000)\n",
    "train_su = su_ids[r3]\n",
    "\n",
    "## randomizing along the time\n",
    "r4 = np.random.randint(0, len(no_ids), 5000)\n",
    "train_no = no_ids[r4]\n",
    "\n",
    "\n",
    "\n",
    "mask_eq = np.ones(eq_ids.shape, dtype = bool)\n",
    "mask_eq[r1] = False\n",
    "\n",
    "mask_px = np.ones(px_ids.shape, dtype = bool)\n",
    "mask_px[r2] = False\n",
    "\n",
    "mask_su = np.ones(su_ids.shape, dtype = bool)\n",
    "mask_su[r3] = False\n",
    "\n",
    "mask_no = np.ones(no_ids.shape, dtype = bool)\n",
    "mask_no[r4] = False\n",
    "\n",
    "test_eq = eq_ids[mask_eq]\n",
    "test_px = px_ids[mask_px]\n",
    "test_su = su_ids[mask_su]\n",
    "test_no = no_ids[mask_no]\n",
    "\n",
    "\n",
    "\n",
    "# concatenating training ids\n",
    "all_train_ids = np.concatenate([train_eq,train_px])\n",
    "\n",
    "# concatenating testing ids\n",
    "all_test_ids = np.concatenate([test_eq,test_px,])\n",
    "\n",
    "# allocating event id as index\n",
    "X_final.index = X_final['event_id'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c80d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a935e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting training and testing values\n",
    "X_train = X_final.loc[all_train_ids]\n",
    "X_test = X_final.loc[all_test_ids]\n",
    "\n",
    "\n",
    "\n",
    "Y_train = X_train['source_type'].values\n",
    "Y_test = X_test['source_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16547ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce9f0ffa",
   "metadata": {},
   "source": [
    "## Performance with manual features added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: without adding anything manual\n",
    "## Check the performance \n",
    "x_train = X_train.iloc[:, 0:int(np.where(X_train.columns == 'serial_no')[0])]\n",
    "x_train = x_train.assign(hod=X_train['hour_of_day'].values, dow=X_train['day_of_week'].values, moy=X_train['month_of_year'].values)\n",
    "\n",
    "x_test = X_test.iloc[:, 0:int(np.where(X_train.columns == 'serial_no')[0])]\n",
    "x_test = x_test.assign(hod=X_test['hour_of_day'].values, dow=X_test['day_of_week'].values, moy=X_test['month_of_year'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdee5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cbac75",
   "metadata": {},
   "source": [
    "## Training on 5000 traces per class and testing on all the remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating a random undersampler\n",
    "rus = RandomUnderSampler(sampling_strategy={'earthquake':5000, 'explosion':5000}, random_state = 42)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Converting the textual labels into numerical labels\n",
    "y_num_test = label_encoder.fit_transform(Y_test)\n",
    "\n",
    "\n",
    "# randomly taking 5000 samples per class from the training dataset\n",
    "X_resampled, y_resampled = rus.fit_resample(x_train, Y_train)\n",
    "\n",
    "\n",
    "# Fit the LabelEncoder on the text labels and transform them to numeric labels\n",
    "y_num_res = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "\n",
    "\n",
    "best_model.class_weight  = None\n",
    "best_model.fit(X_resampled, y_num_res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e2dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e0932b5",
   "metadata": {},
   "source": [
    "## Trace wise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c37e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trace wise performance \n",
    "\n",
    "y_pred = best_model.predict(x_test)\n",
    "plt.style.use('seaborn')\n",
    "trace_cm_phy_tsf_man = confusion_matrix(y_num_test, y_pred)\n",
    "plot_confusion_matrix(trace_cm_phy_tsf_man)\n",
    "\n",
    "# Calculate the classification report\n",
    "trace_report_phy_tsf_man = classification_report(y_num_test, y_pred, output_dict=True)\n",
    "plot_classification_report(trace_report_phy_tsf_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5033bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb28105",
   "metadata": {},
   "source": [
    "## Event Wise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_all = best_model.predict_proba(x_test)\n",
    "\n",
    "X_test['labelled'] = y_num_test\n",
    "X_test['classified'] = y_pred\n",
    "X_test['eq_probability'] = probs_all[:,0]\n",
    "X_test['px_probability'] = probs_all[:,1]\n",
    "\n",
    "\n",
    "\n",
    "mean_labels = X_test.groupby('event_id').mean()['labelled'].values\n",
    "mean_ids = X_test.groupby('event_id').mean().index.values\n",
    "\n",
    "\n",
    "\n",
    "mean_eq_prob = X_test.groupby('event_id').mean()['eq_probability'].values\n",
    "mean_px_prob = X_test.groupby('event_id').mean()['px_probability'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Assigning an event class based on the maximum average probability across the stations. \n",
    "temp_class = np.argmax(np.vstack([mean_eq_prob, mean_px_prob]), axis = 0)\n",
    "## Computing the maximum averaged probability. \n",
    "temp_probs = np.max(np.vstack([mean_eq_prob, mean_px_prob]), axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "cf_events_phy_tsf_man = confusion_matrix(mean_labels, temp_class)\n",
    "plot_confusion_matrix(cf_events_phy_tsf_man)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the classification report\n",
    "report_event_phy_tsf_man = classification_report(mean_labels, temp_class, output_dict=True)\n",
    "plot_classification_report(report_event_phy_tsf_man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4a47c",
   "metadata": {},
   "source": [
    "## Analysis of misclassified events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dda75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities corresponding to misclassified events\n",
    "misclassified_probs = temp_probs[np.where(mean_labels != temp_class)]\n",
    "\n",
    "# classification provided to misclassified events\n",
    "misclassified_class = temp_class[np.where(mean_labels != temp_class)]\n",
    "\n",
    "# labels provided to misclassified events. \n",
    "misclassified_labels = mean_labels[np.where(mean_labels != temp_class)]\n",
    "\n",
    "# event ids provided to misclassified events. \n",
    "misclassified_ids = mean_ids[np.where(mean_labels != temp_class)]\n",
    "\n",
    "# event ids from misclassified events where mean probabilities exceed 0.8\n",
    "misclassified_high_prob = misclassified_ids[np.where(misclassified_probs > 0.8)]\n",
    "\n",
    "\n",
    "# Dataframe of misclassified events\n",
    "misclassified_X = X_test.loc[misclassified_high_prob]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b25a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef19c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242f377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aee95505",
   "metadata": {},
   "source": [
    "## New objectives\n",
    "\n",
    "- download more stations data for each of the misclassified events. \n",
    "- extract features\n",
    "- classify them and see if they still going to be misclassified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ec60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2448f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df88872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_ids = X_test.loc[misclassified_ids].groupby(level=0).size()\n",
    "#indices_with_value_one = misclassified_ids[misclassified_ids == 1].index\n",
    "single_station_event = mis_ids[mis_ids < 3].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e4cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd43ad5d",
   "metadata": {},
   "source": [
    "## Downloading more station waveforms for each of the single event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ddd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[single_station_event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156055ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttimes  = X_test.loc[single_station_event]['trace_start_time'].values\n",
    "source_lats = X_test.loc[single_station_event]['source_latitude_deg'].values\n",
    "source_lons = X_test.loc[single_station_event]['source_longitude_deg'].values\n",
    "event_ids = X_test.loc[single_station_event].index.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc158b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7380c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2bed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2968f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"IRIS\")\n",
    "for i in tqdm(range(len(event_ids))):\n",
    "    \n",
    "    try:\n",
    "\n",
    "\n",
    "        stns = client.get_stations(channel = '*HZ', \n",
    "                                starttime=obspy.UTCDateTime(starttimes[i]),\n",
    "                                endtime= obspy.UTCDateTime(starttimes[i])+150,  \n",
    "                                latitude= source_lats[i], \n",
    "                                longitude = source_lons[i], \n",
    "                                minradius= 0, \n",
    "                                maxradius= 0.3, \n",
    "                                level = 'response'\n",
    "                             )\n",
    "        \n",
    "        for network in stns:\n",
    "            for station in network:\n",
    "                for channel in station:\n",
    "                    st = client.get_waveforms(starttime = obspy.UTCDateTime(starttimes[i]), endtime = obspy.UTCDateTime(starttimes[i]) + 150, network = network.code, station = station.code, channel = channel.code, location = '*')\n",
    "                    st.write('../data/Binary_Classification/'+event_ids[i]+'_'+network.code+'.'+station.code+'.'+channel.code+'.'+str(starttime)+'.mseed')\n",
    "                    \n",
    "                    \n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4188c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3147fcff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b513d2c",
   "metadata": {},
   "source": [
    "## Results after including more stations per waveform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f56a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712e28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260f0328",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f243e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the waveform files in the disk. \n",
    "files = glob('../data/binary_classification_waveforms/*.mseed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2acb12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aa04da4",
   "metadata": {},
   "source": [
    "### Extracting data from each waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "event_ids = []\n",
    "for file in tqdm(files):\n",
    "    st = obspy.read(file)\n",
    "    st.resample(100)\n",
    "    if len(st[0].data) == 15000:\n",
    "        event_ids.append(file.split('/')[-1])   ### .split('_')[0])\n",
    "        \n",
    "        # depending on what features the model was trained on we select the appropriate window. \n",
    "        # The current saved model was trained on the 150s window waveforms (P-50, P+100). So we will load those waveforms\n",
    "        data.append(st[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e6da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the data\n",
    "data = np.stack(data)\n",
    "\n",
    "# applying 10% cosine taper before filtering\n",
    "tapered = apply_cosine_taper(data)\n",
    "\n",
    "# defining the parameters of the filter. \n",
    "lowcut = 1\n",
    "highcut = 10\n",
    "num_corners = 4\n",
    "fs = 100\n",
    "\n",
    "\n",
    "# Applying bandpass filters\n",
    "filtered = butterworth_filter(tapered, lowcut, highcut, fs, num_corners, filter_type='bandpass')\n",
    "\n",
    "# Applying the normalization. \n",
    "norm = filtered/np.max(abs(np.stack(filtered)), axis = 1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978b8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417ad72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33697568",
   "metadata": {},
   "source": [
    "## Extracting tsfel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358eb293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_sttime = time.time()\n",
    "cfg_file = tsfel.get_features_by_domain()\n",
    "# Extract features for surface event\n",
    "features_bin_tsfel = pd.DataFrame([])\n",
    "for i in tqdm(range(len(norm))):\n",
    "\n",
    "        df = time_series_features_extractor(cfg_file, norm[i],  fs=100)\n",
    "        df['event_id'] = event_ids[i]\n",
    "        features_bin_tsfel = pd.concat([features_bin_tsfel,df])\n",
    "\n",
    "print(time.time() - code_sttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55997b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39490473",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time_series_features_extractor(cfg_file, norm[i],  fs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abc120",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e55275f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb627da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8601baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f46d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46d6e90d",
   "metadata": {},
   "source": [
    "## Extracting physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedccaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_sttime = time.time()\n",
    "features_bin_physical = pd.DataFrame([])\n",
    "for i in tqdm(range(len(norm))):\n",
    "    try: \n",
    "        df = seis_feature.FeatureCalculator(norm[i]).compute_features()\n",
    "        df['serial_no'] = i\n",
    "        df['event_id'] = event_ids[i]\n",
    "        #df['trace_id'] = trids[i]\n",
    "        features_bin_physical = pd.concat([features_bin_physical, df])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(time.time() - code_sttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin_physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin_tsfel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d27fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500499a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing the correlated features\n",
    "features_bin_tsfel.drop(corr_features, axis=1, inplace=True, errors = 'ignore')\n",
    "final_features_bin = pd.concat([features_bin_tsfel, features_bin_physical], axis = 1)\n",
    "columns_to_select = scaler_params['Feature'].values\n",
    "features_bin = final_features_bin.loc[:, columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21787b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b2804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f507ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(features_bin.columns)):\n",
    "#    features_bin.iloc[:,i] = (features_bin.iloc[:,i]-scaler_params.iloc[i,1])/(scaler_params.iloc[i,2])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hod_bin = []\n",
    "dow_bin = []\n",
    "moy_bin = []\n",
    "\n",
    "for i in range(len(event_ids)):\n",
    "    value = X_test.loc[event_ids[i]]['hour_of_day']\n",
    "    hod_bin.append(value if isinstance(value, np.float64) else value.values[0])\n",
    "    \n",
    "    value = X_test.loc[event_ids[i]]['day_of_week']\n",
    "    dow_bin.append(value if isinstance(value, np.int64) else value.values[0])\n",
    "    \n",
    "    value = X_test.loc[event_ids[i]]['month_of_year']\n",
    "    moy_bin.append(value if isinstance(value, np.int64) else value.values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf0928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin['hour_of_day'] = hod_bin\n",
    "features_bin['day_of_week'] = dow_bin\n",
    "features_bin['month_of_year'] = moy_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e2425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b4a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6907a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trace wise performance \n",
    "\n",
    "y_pred_bin = best_model.predict(features_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96deaecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin['Eq_prob'] = best_model.predict_proba(features_bin.iloc[:, 0:317])[:,0]\n",
    "features_bin['Exp_prob'] = best_model.predict_proba(features_bin.iloc[:, 0:317])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194a80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d65d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin['event_id'] = event_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_labels_bin = np.argmax(features_bin.groupby('event_id').mean().iloc[:, 318:320].values, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fced89",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ids_mean = features_bin.groupby('event_id').mean().index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce8f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bin[features_bin['event_id'] == event_ids_mean[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045aeaed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0da645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d62cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fac12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10728d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219621f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcfda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5cdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029e4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07550c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_overall = pd.DataFrame([])\n",
    "test_overall['mean_labels'] = mean_labels\n",
    "test_overall['mean_ids'] = mean_ids\n",
    "test_overall['assigned_class'] = temp_class\n",
    "test_overall.index = mean_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2273fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(event_ids_mean)):\n",
    "    test_overall.loc[event_ids_mean[i], 'assigned_class'] = event_labels_bin[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88c35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35d1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_bin = confusion_matrix(test_overall['mean_labels'].values, test_overall['assigned_class'].values)\n",
    "#cf_norm = cf_events/np.sum(cf_events, axis = 1, keepdims = True)\n",
    "class_labels = ['earthquake','explosion']\n",
    "\n",
    "plt.figure(figsize = [8, 6])\n",
    "ax = sns.heatmap(cf_bin, annot = True, cmap='Blues', xticklabels = class_labels, yticklabels = class_labels,  fmt=\"1.0f\", annot_kws=annot_kws)\n",
    "# Set tick label font size\n",
    "ax.set_xticklabels(class_labels, fontsize=15)\n",
    "ax.set_yticklabels(class_labels, fontsize=15)\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize = 15)\n",
    "ax.set_ylabel('Labeled', fontsize = 15)\n",
    "ax.set_title('Total Events: '+str(len(mean_labels)), fontsize = 20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "report_bin = classification_report(test_overall['mean_labels'].values, test_overall['assigned_class'].values, output_dict=True)\n",
    "labels = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "# Set a pleasing style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes for the heatmap\n",
    "plt.figure(figsize = [8,6])\n",
    "ax = sns.heatmap(pd.DataFrame(report_bin).iloc[:3, :2], annot=True, cmap='Blues', yticklabels=labels, xticklabels = class_labels,  vmin=0.8, vmax=1,  annot_kws=annot_kws)\n",
    "\n",
    "# Set tick label font size\n",
    "ax.set_xticklabels(class_labels, fontsize=15)\n",
    "ax.set_yticklabels(labels, fontsize=15)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Metrics', fontsize=15)\n",
    "ax.set_ylabel('Classes', fontsize=15)\n",
    "ax.set_title('Classification Report', fontsize=18)\n",
    "\n",
    "# Create a colorbar\n",
    "#cbar = ax.collections[0].colorbar\n",
    "#cbar.set_ticks([0.5, 1])  # Set custom tick locations\n",
    "#cbar.set_ticklabels(['0', '0.5', '1'])  # Set custom tick labels\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ddf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f94b1c",
   "metadata": {},
   "source": [
    "### Lets try to understand why including more stations is improving the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b6098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c844adc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collecting and storing event ids\n",
    "files = glob('../data/binary_classification_waveforms/*.mseed')\n",
    "station_ids = []\n",
    "for file in tqdm(files):\n",
    "    st = obspy.read(file)\n",
    "    st.resample(100)\n",
    "    if len(st[0].data) == 15000:\n",
    "        station_ids.append(st[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9ec78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bd28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c860d4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c64fe7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 7\n",
    "\n",
    "## Grabbing the data from the stations\n",
    "sample_data = norm[np.where(np.array(event_ids) == event_ids_mean[k])[0]]\n",
    "\n",
    "# Grabbing the station ids \n",
    "sample_station_ids = np.array(station_ids)[np.where(np.array(event_ids) == event_ids_mean[k])[0]]\n",
    "\n",
    "# Grabbing the results\n",
    "sample_df_here = features_bin[features_bin['event_id'] == event_ids_mean[k]]\n",
    "sample_eq_probs = sample_df_here['Eq_prob'].values\n",
    "sample_exp_probs = sample_df_here['Exp_prob'].values\n",
    "\n",
    "\n",
    "# Grabbing the original labels\n",
    "original_label = X_test.loc[event_ids_mean[4]]['labelled']\n",
    "\n",
    "time_here = np.linspace(0, 150, 15000)\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, axs = plt.subplots(nrows=len(sample_data), ncols=1, figsize=(12, 3*len(sample_data)), sharex = True)  # Adjust figsize as needed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot data on each subplot\n",
    "for i in range(len(sample_data)):\n",
    "    axs[i].plot(time_here, sample_data[i], lw=0.5)\n",
    "    #axs[i].set_xlim(40, 80)\n",
    "    axs[i].set_title(sample_station_ids[i]+' Eq prob:'+str(np.around(sample_eq_probs[i],2))+' Exp prob:'+str(np.around(sample_exp_probs[i], 2)))  # Set title for each subplot\n",
    "    #axs[i].set_xlabel('Time(s)')  # Set x-axis label (optional)\n",
    "    axs[i].set_ylabel('Amplitude')  # Set y-axis label (optional)\n",
    "\n",
    "    \n",
    "    \n",
    "# Add common labels to the entire figure\n",
    "fig.suptitle('PNSN Label: '+str(original_label), y = 0.99)  # Title for the entire figure\n",
    "fig.supylabel('Common Y-Axis Label')  # Common y-axis label\n",
    "fig.supxlabel('Time(s)')  # Common x-axis label\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined figure with subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3bf45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c28ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b35764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = scaler_params['Feature'].values\n",
    "#for i in range(len(scaler_params)):\n",
    "#    X_test.loc[:,features[i]] = X_test.loc[:,features[i]]*(scaler_params.iloc[i,2]) + scaler_params.iloc[i,1]\n",
    "    \n",
    "    \n",
    "misclassified_df = X_test.loc[misclassified_ids]\n",
    "misclassified_eq = misclassified_df[misclassified_df['classified'] == 1]\n",
    "misclassified_exp = misclassified_df[misclassified_df['classified'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a644a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cfda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b4f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563bb88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_eq = features_earthquake_physical\n",
    "new_exp = features_explosion_physical\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols = 3, figsize = [20,15])\n",
    "\n",
    "feature = 'DistMaxMedianFreqDTF'\n",
    "\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= features_earthquake_physical[feature].values, bins= num_bins, kde=False, ax = ax[0,0],  \n",
    "             multiple='stack', stat = 'percent', color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= features_explosion_physical[feature].values, bins=num_bins, kde=False, ax = ax[0,0],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "X_miss_earthquake = misclassified_eq[misclassified_eq['px_probability'] > 0.8]\n",
    "X_miss_explosion = misclassified_exp[misclassified_exp['eq_probability'] > 0.8]\n",
    "\n",
    "\n",
    "\n",
    "ax[0,0].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*10, s = 300, ec = 'k', marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[0,0].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*5, s = 300, ec = 'k', marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "\n",
    "\n",
    "#ax[0,0].scatter(X_miss_explosion[feature].values[np.where(probs_miss_explosion > 0.9)[0]], np.ones(len(X_miss_explosion.iloc[np.where(probs_miss_explosion > 0.9)[0]]))*0.5, s = 50, marker = 'o',ec = 'k',c = probs_miss_explosion[np.where(probs_miss_explosion > 0.9)[0]], cmap = 'magma',label = 'Miss. exp')\n",
    "ax[0,0].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "ax[0,0].set_xlim(l1,0.03)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[0,0].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'DistMaxMeanFreqDTF'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[0,1],  \n",
    "             multiple='stack',  stat = 'percent', color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[0,1],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[0,1].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*10,ec = 'k', s = 300, marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[0,1].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*5,ec = 'k',  s = 300, marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[0,1].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "ax[0,1].set_xlim(l1,0.03)\n",
    "\n",
    "\n",
    "ax[0,1].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'E4FFT'\n",
    "\n",
    "l1 = -0.35\n",
    "l2 = -0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int(data_range/ 0.05)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= 2000, kde=False, ax = ax[0,2],  \n",
    "             multiple='stack',  stat = 'percent', color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins= 10, kde=False, ax = ax[0,2],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "\n",
    "ax[0,2].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*40,ec = 'k',  s = 300, marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[0,2].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*20, ec = 'k', s = 300, marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "ax[0,2].legend()\n",
    "#ax[0,2].set_xscale('log')\n",
    "ax[0,2].set_xlim(0,0.05)\n",
    "\n",
    "\n",
    "ax[0,2].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'Gamma1'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[1,0],  \n",
    "             multiple='stack',  stat = 'percent', color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[1,0],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "#ax[1,0].scatter(X_miss_earthquake[feature].values[np.where(probs_miss_earthquake > 0.9)[0]], np.ones(len(X_miss_earthquake.iloc[np.where(probs_miss_earthquake > 0.9)[0]]))*1, s = 50, marker = '^',ec = 'k',c = probs_miss_earthquake[np.where(probs_miss_earthquake > 0.9)[0]], cmap = 'magma',label = 'Miss. eqs')\n",
    "#ax[1,0].scatter(X_miss_explosion[feature].values[np.where(probs_miss_explosion > 0.9)[0]], np.ones(len(X_miss_explosion.iloc[np.where(probs_miss_explosion > 0.9)[0]]))*0.5, s = 50, marker = 'o',ec = 'k',c = probs_miss_explosion[np.where(probs_miss_explosion > 0.9)[0]], cmap = 'magma',label = 'Miss. exp')\n",
    "\n",
    "ax[1,0].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*1, s = 300, ec = 'k', marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[1,0].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*0.5, s = 300, ec = 'k', marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "\n",
    "ax[1,0].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "#ax[0,0].set_xlim(l1,0.02)\n",
    "\n",
    "\n",
    "ax[1,0].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'Gamma2'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[1,1],  \n",
    "             multiple='stack', stat = 'percent', color =  '#1f77b4', alpha = 0.5, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[1,1],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.5, label = 'Explosion')\n",
    "\n",
    "\n",
    "ax[1,1].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*1, s = 300, ec = 'k', marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[1,1].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*0.5, s = 300, ec = 'k', marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "\n",
    "\n",
    "ax[1,1].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "#ax[0,0].set_xlim(l1,0.02)\n",
    "\n",
    "\n",
    "ax[1,1].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = 'Fquart1'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[1,2],  \n",
    "             multiple='stack',  stat = 'percent', color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[1,2],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "\n",
    "ax[1,2].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*1, s = 300, ec = 'k', marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[1,2].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*0.5, s = 300, ec = 'k', marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "ax[1,2].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "#ax[0,0].set_xlim(l1,0.02)\n",
    "\n",
    "\n",
    "ax[1,2].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_eq = features_earthquake_tsfel\n",
    "new_exp = features_explosion_tsfel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = '0_Max power spectrum'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[2,0],  \n",
    "             multiple='stack', stat = 'percent',  color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[2,0],  \n",
    "             multiple='stack', stat = 'percent', color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "\n",
    "\n",
    "ax[2,0].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*10, s = 300,ec = 'k', marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[2,0].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*5, s = 300, ec = 'k', marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "ax[2,0].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "ax[2,0].set_xlim(0,30)\n",
    "\n",
    "\n",
    "ax[2,0].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = '0_Spectral entropy'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[2,1],  \n",
    "             multiple='stack', stat = 'percent',  color =  '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[2,1],  \n",
    "             multiple='stack', stat = 'percent',  color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "\n",
    "ax[2,1].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*4, s = 300, marker = '^',ec = 'k',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[2,1].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*2, s = 300, marker = '*',ec = 'k', c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "\n",
    "ax[2,1].legend()\n",
    "#ax[0,0].set_xscale('log')\n",
    "#ax[0,0].set_xlim(l1,0.02)\n",
    "\n",
    "\n",
    "ax[2,1].set_title(feature, fontsize = 20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature = '0_Median frequency'\n",
    "\n",
    "l1 = 0\n",
    "l2 = 0.1\n",
    "\n",
    "all_df = features_all\n",
    "data_range = all_df[feature].values.ptp()\n",
    "num_bins = int((l2 - l1)/ 0.0005)\n",
    "#sns.histplot(x= no_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0], \n",
    "#             multiple='stack', stat = 'percent', color = 'green', alpha = 0.5,label = 'Noise')\n",
    "#sns.histplot(x= su_df[feature].values, bins= num_bins, kde=False, ax = ax[0,0],   \n",
    "#             multiple='stack', stat = 'percent', color = 'red', alpha = 0.5, label = 'Surface')\n",
    "sns.histplot(x= new_eq[feature].values, bins= num_bins, kde=False, ax = ax[2,2],  \n",
    "             multiple='stack', stat = 'percent',  color = '#1f77b4', alpha = 0.3, label = 'Earthquake')\n",
    "sns.histplot(x= new_exp[feature].values, bins=num_bins, kde=False, ax = ax[2,2],  \n",
    "             multiple='stack', stat = 'percent',  color = '#9467bd', alpha = 0.3, label = 'Explosion')\n",
    "\n",
    "\n",
    "\n",
    "ax[2,2].scatter(X_miss_earthquake[feature].values, np.ones(len(X_miss_earthquake))*1, ec = 'k',s = 300, marker = '^',c = X_miss_earthquake['px_probability'], cmap = 'Reds',label = 'Miss. Eq')\n",
    "ax[2,2].scatter(X_miss_explosion[feature].values, np.ones(len(X_miss_explosion))*0.5, ec = 'k', s = 300, marker = '*',c = X_miss_explosion['eq_probability'], cmap = 'Reds',label = 'Miss. Exp')\n",
    "\n",
    "\n",
    "ax[2,2].legend()\n",
    "#ax[2,2].colorbar()\n",
    "#ax[0,0].set_xscale('log')\n",
    "#ax[0,0].set_xlim(l1,0.02)\n",
    "\n",
    "\n",
    "ax[2,2].set_title(feature, fontsize = 20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82577714",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0a7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d362d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4e953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08050df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roses_2021",
   "language": "python",
   "name": "roses_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
