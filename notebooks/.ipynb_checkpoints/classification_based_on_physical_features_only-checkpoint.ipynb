{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec1b3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns \n",
    "\n",
    "# for converting the text file containing the quarry locations into csv file\n",
    "import csv\n",
    "\n",
    "# for computing the geographical distance between two points \n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, auc, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from datetime import datetime\n",
    "import h5py\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import obspy\n",
    "from obspy.geodetics.base import gps2dist_azimuth, gps2dist_azimuth\n",
    "from obspy.clients.fdsn import Client\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "from joblib import dump, load\n",
    "from obspy.signal.filter import envelope\n",
    "import tsfel\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../feature_extraction_scripts/physical_feature_extraction_scripts')\n",
    "sys.path.append('../common_scripts')\n",
    "import seis_feature\n",
    "#from seis_feature import compute_physical_features\n",
    "from tsfel import time_series_features_extractor, get_features_by_domain\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "\n",
    "from common_processing_functions import apply_cosine_taper\n",
    "from common_processing_functions import butterworth_filter\n",
    "\n",
    "from zenodo_get import zenodo_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b11d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e955159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Title: Physical Features for my study on Automatic Seismic Event Classification System in Pacific Northwest\n",
      "Keywords: \n",
      "Publication date: 2024-02-20\n",
      "DOI: 10.5281/zenodo.10689585\n",
      "Total size: 375.7 MB\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_noise_z_50_100.csv   size: 40.5 MB\n",
      "\n",
      "Checksum is correct. (731c772d47171f5786caf5ee1d34f8b2)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_50_100_part3.csv   size: 38.0 MB\n",
      "\n",
      "Checksum is correct. (3134dad6f8b5fc6472b52ec82e8ed430)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_exotic_z_10_40.csv   size: 7.1 MB\n",
      "\n",
      "Checksum is correct. (f3b2f662e920ce728ccd5eff2a422ea1)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_explosion_50_100.csv   size: 12.4 MB\n",
      "\n",
      "Checksum is correct. (f1db652ed67939ef2c7b62e7eba3895d)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_50_100_part2.csv   size: 38.7 MB\n",
      "\n",
      "Checksum is correct. (295a6629c464d7bace6d0d1e6037ef6e)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_10_40_part2.csv   size: 37.6 MB\n",
      "\n",
      "Checksum is correct. (1f882ab4c894ccdd11e18059df63bddf)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_50_100_part1.csv   size: 19.8 MB\n",
      "\n",
      "Checksum is correct. (f3e09e41676fa391ee052f122a0d3f4e)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_10_40_part1.csv   size: 19.2 MB\n",
      "\n",
      "Checksum is correct. (d04824b99f62fa5fd4b9e8eceb3d82c6)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_50_100_part4.csv   size: 33.7 MB\n",
      "\n",
      "Checksum is correct. (87c91d2269f2510d500ae969eb25ff1e)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_10_40_part3.csv   size: 37.1 MB\n",
      "\n",
      "Checksum is correct. (eec821209fa25757b9ef33276e3b7d2d)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_noise_z_10_40.csv   size: 39.6 MB\n",
      "\n",
      "Checksum is correct. (93a532580d765553d4fb7bf273148869)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_exotic_z_50_100.csv   size: 7.3 MB\n",
      "\n",
      "Checksum is correct. (bdc91fbcb62a1dbc7f1f97eebef0f0ee)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_explosion_10_40.csv   size: 12.1 MB\n",
      "\n",
      "Checksum is correct. (13a18eadb7034ea4737171dcdbb5cd9f)\n",
      "\n",
      "Link: https://zenodo.org/record/10689585/files/physical_features_comcat_z_earthquake_10_40_part4.csv   size: 32.7 MB\n",
      "\n",
      "Checksum is correct. (3757d2a2bbe257fe86197406d164e40b)\n",
      "All files have been downloaded.\n"
     ]
    }
   ],
   "source": [
    "## Downloading the physical features \n",
    "\n",
    "# Define the Zenodo record DOI\n",
    "doi = '10.5281/zenodo.10689585'  ## This is for physical features\n",
    "\n",
    "\n",
    "## This is doi for tsfel features of 50s window length - 10.5281/zenodo.10689673\n",
    "## This is doi for tsfel features of 150s window length - 10.5281/zenodo.10689824\n",
    "\n",
    "# Download files from the Zenodo record\n",
    "files = zenodo_get([doi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6afb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e38f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d29646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7f4d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f9ef63c",
   "metadata": {},
   "source": [
    "## Loading physical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8083ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features of surface events, thunder and sonic booms\n",
    "features_exotic_physical = pd.read_csv('physical_features_exotic_z_50_100.csv')\n",
    "\n",
    "features_surface_physical = features_exotic_physical[features_exotic_physical['source'] == 'surface']\n",
    "features_sonic_physical = features_exotic_physical[features_exotic_physical['source'] == 'sonic']\n",
    "features_thunder_physical = features_exotic_physical[features_exotic_physical['source'] == 'thunder']\n",
    "\n",
    "\n",
    "\n",
    "# features of noise\n",
    "features_noise_physical = pd.read_csv('physical_features_noise_z_50_100.csv')\n",
    "\n",
    "\n",
    "# features of explosion\n",
    "features_explosion_physical = pd.read_csv('physical_features_comcat_z_explosion_50_100.csv')\n",
    "\n",
    "# features of earthquakes\n",
    "features_eq1 = pd.read_csv('physical_features_comcat_z_earthquake_50_100_part1.csv')\n",
    "features_eq2 = pd.read_csv('physical_features_comcat_z_earthquake_50_100_part2.csv')\n",
    "features_eq3 = pd.read_csv('physical_features_comcat_z_earthquake_50_100_part3.csv')\n",
    "features_eq4 = pd.read_csv('physical_features_comcat_z_earthquake_50_100_part4.csv')\n",
    "\n",
    "features_earthquake_physical = pd.concat([features_eq1, features_eq2, features_eq3, features_eq4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1b9dec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we have 240546 events and each event have 64 features\n"
     ]
    }
   ],
   "source": [
    "features_all = pd.concat([features_surface_physical, features_noise_physical, features_explosion_physical, features_earthquake_physical])\n",
    "features_all = features_all.drop(['Unnamed: 0'], axis = 1)\n",
    "print(f'So we have {features_all.shape[0]} events and each event have {features_all.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066e59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec18d20",
   "metadata": {},
   "source": [
    "## Removing the downloaded files to clean up the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ae2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "# List the files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Iterate over the files and delete CSV files\n",
    "for file in files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160942c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ae588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d792e5",
   "metadata": {},
   "source": [
    "## Preprocessing the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a935497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns that contain NaNs\n",
    "features_all = features_all.dropna(axis = 1)\n",
    "\n",
    "# dropping the rows that contains NaNs\n",
    "features_all = features_all.dropna()\n",
    "\n",
    "\n",
    "## dropping all the rows containing infinity values\n",
    "features_all = features_all.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "\n",
    "## dropping sonic boom and thunder events\n",
    "features_all = features_all[features_all['source'] != 'sonic']\n",
    "features_all = features_all[features_all['source'] != 'thunder']\n",
    "\n",
    "print(f'So we have {features_all.shape[0]} events and each event have {features_all.shape[1]} features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5a556",
   "metadata": {},
   "source": [
    "## Dropping the columns that contain same values for all the events, as these features do not really contribute in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b21697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in each column\n",
    "unique_counts = features_all.nunique()\n",
    "\n",
    "# Identify columns with only one unique value (same value for all rows)\n",
    "single_value_columns = unique_counts[unique_counts == 1].index\n",
    "\n",
    "\n",
    "# Drop columns with the same value for all rows\n",
    "features_all = features_all.drop(columns=single_value_columns)\n",
    "\n",
    "print(f'So we have {features_all.shape[0]} events and each event have {features_all.shape[1]} features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c5527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b0ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4979fdc9",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = features_all.drop(['serial_no', 'source'], axis = 1)\n",
    "# Calculate Z-scores for each feature\n",
    "z_scores = np.abs(stats.zscore(df))\n",
    "\n",
    "\n",
    "# Define a threshold for Z-score beyond which data points are considered outliers\n",
    "threshold = 4\n",
    "\n",
    "# Filter out rows with any Z-score greater than the threshold\n",
    "# Temporarily removing this \n",
    "outliers_removed_df =   features_all[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "print(len(features_all))\n",
    "print(len(outliers_removed_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3baac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'So we now have {outliers_removed_df.shape[0]} events and each event have {outliers_removed_df.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b63476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3e6611b",
   "metadata": {},
   "source": [
    "## Standardizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the global variables X and y\n",
    "X = outliers_removed_df.drop(['serial_no','source'], axis = 1)\n",
    "y = outliers_removed_df['source']\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "# Apply standard scaling to the DataFrame\n",
    "scaled_features = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Access the mean and standard deviation for each feature\n",
    "means = scaler.mean_\n",
    "std_devs = scaler.scale_\n",
    "\n",
    "# Create a DataFrame to display the means and standard deviations\n",
    "scaler_params = pd.DataFrame({'Feature': X.columns, 'Mean': means, 'Std Dev': std_devs})\n",
    "print(scaler_params)\n",
    "\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "X_scaled = pd.DataFrame(scaled_features, columns=X.columns)\n",
    "\n",
    "\n",
    "## We are not standardizing at this stage. We will rather wait when the outlier are removed, then we will\n",
    "## standardize and save the standard scaler parameters. \n",
    "#X_scaled = X\n",
    "\n",
    "X_scaled['serial_no'] = outliers_removed_df['serial_no'].values\n",
    "X_scaled['source'] = outliers_removed_df['source'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a83fa",
   "metadata": {},
   "source": [
    "## Merging the metadata information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77abdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the stored data\n",
    "comcat_file_name = h5py.File(\"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\",'r')\n",
    "exotic_file_name = h5py.File(\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\",'r')\n",
    "noise_file_name = h5py.File(\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\",'r')\n",
    "\n",
    "\n",
    "# extracting the catalog\n",
    "comcat_file_csv = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "exotic_file_csv = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "noise_file_csv = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# extracting the metadata corresponding to individual events\n",
    "cat_exp = comcat_file_csv[comcat_file_csv['source_type'] == 'explosion']\n",
    "cat_eq = comcat_file_csv[comcat_file_csv['source_type'] == 'earthquake']\n",
    "cat_no = noise_file_csv\n",
    "cat_su = exotic_file_csv[exotic_file_csv['source_type'] == 'surface event']\n",
    "\n",
    "\n",
    "\n",
    "# extracting the index \n",
    "ind_exp = X_scaled[X_scaled['source'] == 'explosion']['serial_no'].values\n",
    "ind_eq = X_scaled[X_scaled['source'] == 'earthquake']['serial_no'].values\n",
    "ind_no = X_scaled[X_scaled['source'] == 'noise']['serial_no'].values\n",
    "ind_su = X_scaled[X_scaled['source'] == 'surface']['serial_no'].values\n",
    "\n",
    "\n",
    "df_exp = X_scaled[X_scaled['source'] == 'explosion']\n",
    "exp_df = cat_exp.iloc[ind_exp]\n",
    "exp_df['serial_no'] = ind_exp\n",
    "\n",
    "\n",
    "df_eq = X_scaled[X_scaled['source'] == 'earthquake']\n",
    "eq_df = cat_eq.iloc[ind_eq]\n",
    "eq_df['serial_no'] = ind_eq\n",
    "\n",
    "\n",
    "\n",
    "df_no = X_scaled[X_scaled['source'] == 'noise']\n",
    "no_df = cat_no.iloc[ind_no]\n",
    "no_df['serial_no'] = ind_no\n",
    "\n",
    "\n",
    "\n",
    "df_su = X_scaled[X_scaled['source'] == 'surface']\n",
    "su_df = cat_su.iloc[ind_su]\n",
    "su_df['serial_no'] = ind_su\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_exp = pd.merge(df_exp,exp_df, on = 'serial_no')\n",
    "new_eq = pd.merge(df_eq,eq_df, on = 'serial_no')\n",
    "new_su = pd.merge(df_su,su_df, on = 'serial_no')\n",
    "new_no = pd.merge(df_no,no_df, on = 'serial_no')\n",
    "new_no['event_id'] = np.array(['noise'+str(i) for i in np.arange(len(new_no))])\n",
    "\n",
    "\n",
    "\n",
    "X_final = pd.concat([new_exp, new_eq, new_su, new_no])\n",
    "y = ['explosion']*len(new_exp)+['earthquake']*len(new_eq)+['surface']*len(new_su)+['noise']*len(new_no)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af390fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39d4b2b7",
   "metadata": {},
   "source": [
    "### Adding Hour of the Day, Days of Week and Month of Year as additional features as they have shown to significantly increase the performance of the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f08462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_exp contains the features and the corresponding metadata information. \n",
    "datetimes = X_final['trace_start_time'].values\n",
    "\n",
    "hour_of_day = []\n",
    "days_of_week = []\n",
    "month_of_year = []\n",
    "for dt_str in tqdm(datetimes):\n",
    "        \n",
    "    # Parse the datetime string\n",
    "        dt = datetime.strptime(dt_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        hod = dt.hour - 8.  # converting to local time. \n",
    "        moy = dt.month\n",
    "        \n",
    "        \n",
    "        days_of_week.append(dt.weekday())\n",
    "        hour_of_day.append(hod)\n",
    "        month_of_year.append(moy)\n",
    "        \n",
    "X_final['hour_of_day'] = hour_of_day\n",
    "X_final['day_of_week'] = days_of_week\n",
    "X_final['month_of_year'] = month_of_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c20a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36867100",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X = X_final.iloc[:,0:60]\n",
    "#temp_X = temp_X.assign(hod=X_final['hour_of_day'].values, dow=X_final['day_of_week'].values, moy=X_final['month_of_year'].values)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply standard scaling to the DataFrame\n",
    "scaled_features = scaler.fit_transform(temp_X)\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "temp_X = pd.DataFrame(scaled_features, columns= temp_X.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f83ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9688917",
   "metadata": {},
   "source": [
    "## So X_final that contains the features and corresponding metadata information for all the events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e291c",
   "metadata": {},
   "source": [
    "### hyperparameter tuning of the model based on 3000 samples per class and splitting into 80-20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random undersampling using imbalanced-learn library\n",
    "rus = RandomUnderSampler(sampling_strategy={'earthquake':3000, 'explosion':3000, 'surface':3000, 'noise':3000})\n",
    "X_resampled, y_resampled = rus.fit_resample(temp_X, y)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the text labels and transform them to numeric labels\n",
    "y_num = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_num, test_size=0.2, stratify = y_num)\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid for randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize the RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model, param_distributions=param_dist, n_iter=50, scoring='f1_macro', cv=10, verbose=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform randomized grid search cross-validation\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and their corresponding accuracy score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy:\", random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00af18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d438cc",
   "metadata": {},
   "source": [
    "### Performance with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee14916",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance with best model\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "labels = ['Earthquake', 'Explosion','Noise','Surface']\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels = labels, yticklabels = labels)\n",
    "plt.xlabel('Predicted', fontsize = 15)\n",
    "plt.ylabel('Actual', fontsize = 15)\n",
    "plt.title('Total samples: '+str(len(y_pred)), fontsize = 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b126f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f3b5a0",
   "metadata": {},
   "source": [
    "### Classification report with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "labels = ['Earthquake', 'Explosion', 'Noise','Surface']\n",
    "\n",
    "# Set a pleasing style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes for the heatmap\n",
    "plt.figure()\n",
    "ax = sns.heatmap(pd.DataFrame(report).iloc[:3, :4], annot=True, cmap='Blues', xticklabels=labels, vmin=0.5, vmax=1)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Metrics', fontsize=15)\n",
    "ax.set_ylabel('Classes', fontsize=15)\n",
    "ax.set_title('Classification Report', fontsize=18)\n",
    "\n",
    "# Create a colorbar\n",
    "#cbar = ax.collections[0].colorbar\n",
    "#cbar.set_ticks([0.5, 1])  # Set custom tick locations\n",
    "#cbar.set_ticklabels(['0', '0.5', '1'])  # Set custom tick labels\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e057fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b31af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7074db42",
   "metadata": {},
   "source": [
    "## In the following code, we are randomly selecting 5000 \"Events\" per class,  and separating them from the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11476a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting metadata information for each kind of source along with features\n",
    "a_eq = X_final[X_final['source_type_pnsn_label'] == 'eq']\n",
    "a_px = X_final[X_final['source_type_pnsn_label'] == 'px']\n",
    "a_su = X_final[X_final['source_type'] == 'surface event']\n",
    "a_no = X_final[X_final['source_type'] == 'noise']\n",
    "\n",
    "\n",
    "\n",
    "## extracting the event ids corresponding to each catalog\n",
    "eq_ids = np.unique(a_eq['event_id'].values)\n",
    "su_ids = np.unique(a_su['event_id'].values)\n",
    "no_ids = np.unique(a_no['event_id'].values)\n",
    "px_ids = np.unique(a_px['event_id'].values)\n",
    "\n",
    "\n",
    "## defining the events for training and testing in 70:30 ratio\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Modifying this section a bit, for a fair comparison to deep neural network. \n",
    "\n",
    "\n",
    "train_eq = eq_ids[0:int(0.7*len(eq_ids))]\n",
    "train_px = px_ids[0:int(0.7*len(px_ids))]\n",
    "train_su = su_ids[0:int(0.7*len(su_ids))]\n",
    "train_no = no_ids[0:int(0.7*len(no_ids))]\n",
    "\n",
    "\n",
    "test_eq = eq_ids[int(0.7*len(eq_ids)):len(eq_ids)]\n",
    "test_px = px_ids[int(0.7*len(px_ids)):len(px_ids)]\n",
    "test_su = su_ids[int(0.7*len(su_ids)):len(su_ids)]\n",
    "test_no = no_ids[int(0.7*len(no_ids)):len(no_ids)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "## randomizing along the time. \n",
    "r1 = np.random.randint(0, len(eq_ids), 5000)\n",
    "train_eq = eq_ids[r1]\n",
    "\n",
    "## randomizing along the time. \n",
    "r2 = np.random.randint(0, len(px_ids), 5000)\n",
    "train_px = px_ids[r2]\n",
    "\n",
    "## randomizing along the time. \n",
    "r3 = np.random.randint(0, len(su_ids), 5000)\n",
    "train_su = su_ids[r3]\n",
    "\n",
    "## randomizing along the time\n",
    "r4 = np.random.randint(0, len(no_ids), 5000)\n",
    "train_no = no_ids[r4]\n",
    "\n",
    "\n",
    "\n",
    "mask_eq = np.ones(eq_ids.shape, dtype = bool)\n",
    "mask_eq[r1] = False\n",
    "\n",
    "mask_px = np.ones(px_ids.shape, dtype = bool)\n",
    "mask_px[r2] = False\n",
    "\n",
    "mask_su = np.ones(su_ids.shape, dtype = bool)\n",
    "mask_su[r3] = False\n",
    "\n",
    "mask_no = np.ones(no_ids.shape, dtype = bool)\n",
    "mask_no[r4] = False\n",
    "\n",
    "test_eq = eq_ids[mask_eq]\n",
    "test_px = px_ids[mask_px]\n",
    "test_su = su_ids[mask_su]\n",
    "test_no = no_ids[mask_no]\n",
    "\n",
    "\n",
    "\n",
    "# concatenating training ids\n",
    "all_train_ids = np.concatenate([train_eq,train_px, train_su, train_no])\n",
    "\n",
    "# concatenating testing ids\n",
    "all_test_ids = np.concatenate([test_eq,test_px, test_su, test_no])\n",
    "\n",
    "# allocating event id as index\n",
    "X_final.index = X_final['event_id'].values\n",
    "\n",
    "\n",
    "# extracting training and testing values\n",
    "X_train = X_final.loc[all_train_ids]\n",
    "X_test = X_final.loc[all_test_ids]\n",
    "\n",
    "\n",
    "\n",
    "Y_train = X_train['source_type'].values\n",
    "Y_test = X_test['source_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e66eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: without adding anything manual\n",
    "## Check the performance \n",
    "x_train = X_train.iloc[:, 0:60]\n",
    "#x_train = x_train.assign(hod=X_train['hour_of_day'].values, dow=X_train['day_of_week'].values, moy=X_train['month_of_year'].values)\n",
    "\n",
    "x_test = X_test.iloc[:, 0:60]\n",
    "#x_test = x_test.assign(hod=X_test['hour_of_day'].values, dow=X_test['day_of_week'].values, moy=X_test['month_of_year'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad5ce9f",
   "metadata": {},
   "source": [
    "## Training the model on 5000 randomly samples waveforms per class and testing it on the remaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating a random undersampler\n",
    "rus = RandomUnderSampler(sampling_strategy={'earthquake':5000, 'explosion':5000,'surface event':5000,'noise':5000}, random_state = 42)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Converting the textual labels into numerical labels\n",
    "y_num_test = label_encoder.fit_transform(Y_test)\n",
    "\n",
    "\n",
    "# randomly taking 5000 samples per class from the training dataset\n",
    "X_resampled, y_resampled = rus.fit_resample(x_train, Y_train)\n",
    "\n",
    "\n",
    "# Fit the LabelEncoder on the text labels and transform them to numeric labels\n",
    "y_num_res = label_encoder.fit_transform(y_resampled)\n",
    "\n",
    "\n",
    "\n",
    "best_model.class_weight  = None\n",
    "best_model.fit(X_resampled, y_num_res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac4a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0fdc4e4",
   "metadata": {},
   "source": [
    "## Trace wise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(x_test)\n",
    "plt.style.use('seaborn')\n",
    "trace_cm_phy = confusion_matrix(y_num_test, y_pred)\n",
    "\n",
    "\n",
    "labels = ['Earthquake', 'Explosion','Noise','Surface']\n",
    "plt.figure()\n",
    "sns.heatmap(trace_cm_phy, annot=True, cmap='Blues', fmt='d', xticklabels = labels, yticklabels = labels)\n",
    "plt.xlabel('Predicted', fontsize = 15)\n",
    "plt.ylabel('Actual', fontsize = 15)\n",
    "plt.title('Total samples: '+str(len(y_pred)), fontsize = 20)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the classification report\n",
    "trace_report_phy = classification_report(y_num_test, y_pred, output_dict=True)\n",
    "labels = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "# Set a pleasing style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes for the heatmap\n",
    "plt.figure()\n",
    "ax = sns.heatmap(pd.DataFrame(trace_report_phy).iloc[:3, :4], annot=True, cmap='Blues', xticklabels=labels, vmin=0.8, vmax=1)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Metrics', fontsize=15)\n",
    "ax.set_ylabel('Classes', fontsize=15)\n",
    "ax.set_title('Classification Report', fontsize=18)\n",
    "\n",
    "# Create a colorbar\n",
    "#cbar = ax.collections[0].colorbar\n",
    "#cbar.set_ticks([0.5, 1])  # Set custom tick locations\n",
    "#cbar.set_ticklabels(['0', '0.5', '1'])  # Set custom tick labels\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366de30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e44a832a",
   "metadata": {},
   "source": [
    "## Event wise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_all = best_model.predict_proba(x_test)\n",
    "\n",
    "X_test['labelled'] = y_num_test\n",
    "X_test['classified'] = y_pred\n",
    "X_test['eq_probability'] = probs_all[:,0]\n",
    "X_test['px_probability'] = probs_all[:,1]\n",
    "X_test['no_probability'] = probs_all[:,2]\n",
    "X_test['su_probability'] = probs_all[:,3]\n",
    "\n",
    "\n",
    "mean_labels = X_test.groupby('event_id').mean()['labelled'].values\n",
    "mean_ids = X_test.groupby('event_id').mean().index.values\n",
    "\n",
    "\n",
    "\n",
    "mean_eq_prob = X_test.groupby('event_id').mean()['eq_probability'].values\n",
    "mean_px_prob = X_test.groupby('event_id').mean()['px_probability'].values\n",
    "mean_no_prob = X_test.groupby('event_id').mean()['no_probability'].values\n",
    "mean_su_prob = X_test.groupby('event_id').mean()['su_probability'].values\n",
    "\n",
    "\n",
    "\n",
    "temp_class = np.argmax(np.vstack([mean_eq_prob, mean_px_prob, mean_no_prob, mean_su_prob]), axis = 0)\n",
    "temp_probs = np.max(np.vstack([mean_eq_prob, mean_px_prob, mean_no_prob, mean_su_prob]), axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "cf_events_phy = confusion_matrix(mean_labels, temp_class)\n",
    "cf_norm = cf_events_phy/np.sum(cf_events_phy, axis = 1, keepdims = True)\n",
    "labels = ['earthquake','explosion','noise','surface events']\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cf_events_phy, annot = True, cmap='Blues', xticklabels = labels, yticklabels = labels,  fmt=\"1.0f\")\n",
    "ax.set_xlabel('Predicted', fontsize = 12)\n",
    "ax.set_ylabel('Labeled', fontsize = 12)\n",
    "ax.set_title('Total Events: '+str(len(mean_labels)), fontsize = 20)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the classification report\n",
    "report_event_phy = classification_report(mean_labels, temp_class, output_dict=True)\n",
    "labels = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "# Set a pleasing style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes for the heatmap\n",
    "plt.figure()\n",
    "ax = sns.heatmap(pd.DataFrame(report_event_phy).iloc[:3, :4], annot=True, cmap='Blues', xticklabels=labels, vmin=0.8, vmax=1)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Metrics', fontsize=15)\n",
    "ax.set_ylabel('Classes', fontsize=15)\n",
    "ax.set_title('Classification Report', fontsize=18)\n",
    "\n",
    "# Create a colorbar\n",
    "#cbar = ax.collections[0].colorbar\n",
    "#cbar.set_ticks([0.5, 1])  # Set custom tick locations\n",
    "#cbar.set_ticklabels(['0', '0.5', '1'])  # Set custom tick labels\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26be69e",
   "metadata": {},
   "source": [
    "## Saving results of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a324b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving every result into disk\n",
    "\n",
    "# Saving trace results\n",
    "\n",
    "## physical \n",
    "# Save to a file\n",
    "with open('../Results/trace_report_phy.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(trace_report_phy, pickle_file)\n",
    "\n",
    "    \n",
    "# Save to a file\n",
    "with open('../Results/trace_confusion_matrix_phy.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(trace_cm_phy, pickle_file)\n",
    "    \n",
    "    \n",
    "\n",
    "# Saving event results\n",
    "\n",
    "with open('../Results/event_report_phy.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(report_event_phy, pickle_file)\n",
    "\n",
    "    \n",
    "\n",
    "with open('../Results/event_confusion_matrix_phy.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(cf_events_phy, pickle_file)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410ba7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cd227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214781af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roses_2021",
   "language": "python",
   "name": "roses_2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
