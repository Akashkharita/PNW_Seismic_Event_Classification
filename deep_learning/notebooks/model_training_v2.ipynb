{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "619ec91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# === Standard Libraries ===\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "# === Scientific Libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# === Signal Processing ===\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, correlate\n",
    "\n",
    "# === Seismology Libraries ===\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# === Machine Learning Libraries ===\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === File Handling ===\n",
    "import h5py\n",
    "\n",
    "# === Custom Modules ===\n",
    "module_path = os.path.abspath(os.path.join('../scripts'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neural_network_architectures import (\n",
    "     QuakeXNet_1d, QuakeXNet_2d, SeismicCNN_1d, SeismicCNN_2d )\n",
    "\n",
    "\n",
    "# === Seismology Client ===\n",
    "client = Client('IRIS')\n",
    "\n",
    "from utils import extract_waveforms\n",
    "from utils import compute_spectrogram\n",
    "from utils import normalize_spectrogram_minmax\n",
    "from utils import return_train_val_loaders\n",
    "from utils import plot_confusion_matrix_and_cr\n",
    "from utils import train_model\n",
    "from utils import WaveformPreprocessor\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17940c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a3ccdf",
   "metadata": {},
   "source": [
    "## Training different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6f3ee",
   "metadata": {},
   "source": [
    "## I downloaded additional surface event data, the step below is just processing those additionally downloaded waveforms and these will be added later to the the total dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136176f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nos = 2000\n",
    "\n",
    "# if we are taking all data or not. \n",
    "all_data = False\n",
    "\n",
    "# the start point will be selected randomly from (start, -4)\n",
    "start = - 40\n",
    "shifting = True\n",
    "\n",
    "# training parameters\n",
    "train_split = 70                                      \n",
    "val_split=20\n",
    "test_split = 10\n",
    "learning_rate=0.001\n",
    "batch_size=128\n",
    "n_epochs=60\n",
    "dropout=0.4\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_channels = 3\n",
    "# new sampling rate\n",
    "fs = 50\n",
    "\n",
    "## filtering parameters\n",
    "highcut = 20\n",
    "lowcut = 1\n",
    "input_window_length = 100\n",
    "\n",
    "# randomly starting between -40 to -5s\n",
    "start = -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3b01bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|██████████| 6495/6495 [00:11<00:00, 581.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_surface_events(data_path, ids_path, fs=50, original_fs = 100, lowcut=1, highcut=20, window_length=100, taper_alpha=0.1, random_offset=(-40, -5)):\n",
    "    \"\"\"\n",
    "    Processes surface event data by applying tapering, bandpass filtering, resampling, and normalization.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the surface event data (.npy file).\n",
    "        ids_path (str): Path to the surface event IDs (JSON file).\n",
    "        fs (int): Sampling rate for resampling.\n",
    "        lowcut (float): Low cutoff frequency for bandpass filter.\n",
    "        highcut (float): High cutoff frequency for bandpass filter.\n",
    "        window_length (int): Length of the waveform window (in seconds).\n",
    "        taper_alpha (float): Alpha value for the Tukey window.\n",
    "        random_offset (tuple): Range of random offsets for slicing data.\n",
    "        \n",
    "    Returns:\n",
    "        list: Processed waveform data.\n",
    "        list: Corresponding event IDs.\n",
    "    \"\"\"\n",
    "    # Load data and IDs\n",
    "    surface_data = np.load(data_path, allow_pickle=True)\n",
    "    with open(ids_path, \"r\") as file:\n",
    "        surface_ids = json.load(file)\n",
    "\n",
    "\n",
    "    processed_data = []\n",
    "    processed_ids = []\n",
    "\n",
    "    # Process each event\n",
    "    for i in tqdm(range(len(surface_data)), desc=\"Processing events\"):\n",
    "        try:\n",
    "            event_data = surface_data[i]\n",
    "            \n",
    "            orig_fs = 100\n",
    "            \n",
    "            # Randomly select a window of the specified length\n",
    "            random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "            \n",
    "            # assuming the onset at 90s\n",
    "            start_idx = int(90 * orig_fs) + random_shift\n",
    "            end_idx = start_idx + int(window_length * orig_fs)\n",
    "            \n",
    "    \n",
    "            # Handle boundary conditions\n",
    "            max_idx = event_data.shape[-1]\n",
    "            if end_idx > max_idx:\n",
    "                end_idx = max_idx\n",
    "                start_idx = end_idx - int(window_length * orig_fs)\n",
    "            if start_idx < 0:\n",
    "                start_idx = 0\n",
    "                end_idx = int(window_length * orig_fs)\n",
    "                \n",
    "                \n",
    "            sliced = event_data[:, start_idx:end_idx]\n",
    "            sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            processor = WaveformPreprocessor(\n",
    "            input_fs=original_fs,\n",
    "            target_fs=fs,\n",
    "            lowcut=lowcut,\n",
    "            highcut=highcut)\n",
    "            \n",
    "            processed = processor(sliced_tensor)  # (C, T)\n",
    "          \n",
    "        \n",
    "            if processed.shape[-1] != int(window_length*fs):\n",
    "                print('error')\n",
    "                continue\n",
    "\n",
    "            x = processed.numpy()\n",
    "            \n",
    "            if len(x) == 3:  # Ensure the event has three components\n",
    "                processed_data.append(x)\n",
    "                processed_ids.append(surface_ids[i])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log or print the exception if needed\n",
    "            print(f\"Error processing event {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return processed_data, processed_ids\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_path = '../../data/new_curated_surface_event_data.npy'\n",
    "ids_path = '../../data/new_curated_surface_event_ids.json'\n",
    "\n",
    "processed, processed_id = process_surface_events(data_path, ids_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147178d",
   "metadata": {},
   "source": [
    "## Loading the stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "705a93f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data files\n",
    "file_noise=\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\";\n",
    "file_comcat=  \"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\";\n",
    "file_exotic=\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\";\n",
    "\n",
    "# metadata\n",
    "# accessing the comcat metadata\n",
    "comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "# comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "\n",
    "# accessing the exotic metadata\n",
    "exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "# exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "\n",
    "# accessing the data files\n",
    "metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "# metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "# creating individual data frames for each class\n",
    "cat_exp = comcat_metadata[comcat_metadata['source_type'] == 'explosion']\n",
    "cat_eq = comcat_metadata[comcat_metadata['source_type'] == 'earthquake']\n",
    "cat_su = exotic_metadata[exotic_metadata['source_type'] == 'surface event']\n",
    "cat_noise = metadata_noise\n",
    "cat_noise['event_id'] = [cat_noise['trace_start_time'][i]+'_noise' for i in range(len(cat_noise))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe0a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the threshold\n",
    "SNR_THR = 1\n",
    "\n",
    "# explosions\n",
    "trace_snr_db_values = np.array([float(cat_exp.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_exp.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_exp = cat_exp.iloc[ii2]\n",
    "\n",
    "# earthquake\n",
    "trace_snr_db_values = np.array([float(cat_eq.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_eq.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_eq = cat_eq.iloc[ii2]\n",
    "\n",
    "# surface events\n",
    "trace_snr_db_values = np.array([float(cat_su.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_su.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR-2)[0].astype(int) \n",
    "df_su = cat_su.iloc[ii2]\n",
    "\n",
    "# noise\n",
    "# does not change\n",
    "df_noise = cat_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9197e",
   "metadata": {},
   "source": [
    "## Collecting three component waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d959d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8434/8434 [00:20<00:00, 421.18it/s]\n",
      "  0%|          | 10/15000 [00:00<02:38, 94.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3778, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:30<00:00, 99.74it/s] \n",
      "  0%|          | 7/13638 [00:00<03:26, 66.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10644, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13638/13638 [03:02<00:00, 74.58it/s]\n",
      "  0%|          | 0/17000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8749, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [03:54<00:00, 72.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10474, 3, 5000)\n"
     ]
    }
   ],
   "source": [
    "number_data_per_class = len(df_su)\n",
    "# surface events\n",
    "d_su, id_su = extract_waveforms(df_su, file_exotic, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_su.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = 15000\n",
    "# noise\n",
    "d_noise, id_noise = extract_waveforms(df_noise, file_noise, input_window_length = input_window_length, fs=fs,\n",
    "                                      start = start, number_data = number_data_per_class,\n",
    "                                      num_channels = num_channels, shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_noise.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = len(df_exp)\n",
    "# explosions\n",
    "d_exp, id_exp = extract_waveforms(df_exp, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                  start = start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                  shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "\n",
    "print(d_exp.shape)\n",
    "\n",
    "\n",
    "number_data_per_class = 17000\n",
    "# earthquakes\n",
    "d_eq, id_eq = extract_waveforms(df_eq, file_comcat, input_window_length = input_window_length,  fs=fs,\n",
    "                                start =start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_eq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f7681",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Now we will add the newly downloaded surface events data to the existing three channel surface events. This will give us about 10k waveforms per class. So total 40k waveforms per class, we will divide these into test and train, save the test. This test dataset will be commonly evaluated across DL and ML algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d0e23b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_waveforms: (7750, 3, 5000)\n",
      "Shape of train_waveforms: (30999, 3, 5000)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(d_eq, d_exp, d_noise, d_su, processed, processed_ids, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Prepares and concatenates seismic data for training and testing.\n",
    "    \n",
    "    Args:\n",
    "        d_eq, d_exp, d_noise, d_su (array): Arrays of waveform data for different classes.\n",
    "        processed (array): Processed surface waveforms.\n",
    "        processed_ids (array): Corresponding IDs for processed surface waveforms.\n",
    "        num_samples (int): Number of samples to extract per class.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined waveforms (new_X_1d), event IDs (new_X), and labels (new_y).\n",
    "    \"\"\"\n",
    "    # Extract the required number of samples per class\n",
    "    new_d_eq = d_eq[:num_samples]\n",
    "    new_d_exp = d_exp[:num_samples]\n",
    "    new_d_no = d_noise[:num_samples]\n",
    "    new_d_su = np.concatenate([d_su, np.array(processed)])[:num_samples]\n",
    "\n",
    "    # Extract corresponding event IDs\n",
    "    new_id_eq = id_eq[:num_samples]\n",
    "    new_id_exp = id_exp[:num_samples]\n",
    "    new_id_no = id_noise[:num_samples]\n",
    "    new_id_su = np.concatenate([id_su, np.array(processed_ids)])[:num_samples]\n",
    "\n",
    "    # Combine data into single arrays\n",
    "    new_X_1d = np.vstack([new_d_eq, new_d_exp, new_d_no, new_d_su])  # Waveforms\n",
    "    new_X = np.hstack([new_id_eq, new_id_exp, new_id_no, new_id_su])  # Event IDs\n",
    "    new_y = (\n",
    "        [0] * len(new_d_eq) + \n",
    "        [1] * len(new_d_exp) + \n",
    "        [2] * len(new_d_no) + \n",
    "        [3] * len(new_d_su)\n",
    "    )  # Labels\n",
    "    \n",
    "    return new_X_1d, new_X, new_y\n",
    "\n",
    "def split_and_save_data(new_X, new_X_1d, new_y, test_size=0.2, random_state=42, save_path=\"../../data\"):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing, and saves the results to disk.\n",
    "    \n",
    "    Args:\n",
    "        new_X (array): Event IDs.\n",
    "        new_X_1d (array): Waveform data.\n",
    "        new_y (list): Labels for the event IDs.\n",
    "        test_size (float): Proportion of test data.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        save_path (str): Directory to save the test data IDs.\n",
    "    \"\"\"\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        new_X, new_y, test_size=test_size, random_state=random_state, stratify=new_y\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of event IDs to indices\n",
    "    event_id_to_index = {event_id: idx for idx, event_id in enumerate(new_X)}\n",
    "    \n",
    "    # Retrieve indices for test and train data\n",
    "    test_indices = [event_id_to_index[event_id] for event_id in X_test]\n",
    "    train_indices = [event_id_to_index[event_id] for event_id in X_train]\n",
    "    \n",
    "    # Extract waveforms for train and test sets\n",
    "    test_waveforms = new_X_1d[test_indices]\n",
    "    train_waveforms = new_X_1d[train_indices]\n",
    "    \n",
    "    # Save test data IDs for future use\n",
    "    np.save(f\"{save_path}/common_test_data_id.npy\", X_test)\n",
    "    np.save(f\"{save_path}/common_test_data_for_deep_learning.npy\", test_waveforms)\n",
    "    np.save(f\"{save_path}/common_test_data_labels_for_deep_learning.npy\", y_test)\n",
    "    \n",
    "    print(\"Shape of test_waveforms:\", test_waveforms.shape)\n",
    "    print(\"Shape of train_waveforms:\", train_waveforms.shape)\n",
    "\n",
    "    return train_waveforms, test_waveforms, y_train, y_test\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "new_X_1d, new_X, new_y = prepare_data(d_eq, d_exp, d_noise, d_su, processed, processed_id, num_samples=10000)\n",
    "train_waveforms, test_waveforms, y_train, y_test = split_and_save_data(new_X, new_X_1d, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d72c9",
   "metadata": {},
   "source": [
    "## Data Augmentation Strategy\n",
    "We began with approximately 40,000 waveform samples, evenly distributed across four classes: earthquakes, explosions, surface events, and noise. The dataset was initially split into:\n",
    "\n",
    "Training set: 80% → 32,000 samples\n",
    "\n",
    "Test set: 20% → 8,000 samples\n",
    "\n",
    "To improve model robustness and generalization, we applied additive noise augmentation to the training set. Specifically, for each sample in the earthquake, explosion, and surface event classes, we added scaled noise traces sampled from the noise class.\n",
    "\n",
    "This resulted in:\n",
    "\n",
    "Augmented training set: Original 32,000 samples → 64,000 samples (with noise-augmented duplicates)\n",
    "\n",
    "We then performed a train/validation split on the augmented training set:\n",
    "\n",
    "Training set: 75% → 48,000 samples (~12,000 per class)\n",
    "\n",
    "Validation set: 25% → 16,000 samples (~4,000 per class)\n",
    "\n",
    "## Observations\n",
    "This augmentation strategy significantly improved the performance of the 1D convolutional model.\n",
    "\n",
    "The 1D model’s accuracy now closely matches that of the 2D (spectrogram-based) model, suggesting that robustness—not necessarily input type—was a limiting factor prior to augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d28f43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_noise_numpy(data, noise_pool, scale = 1, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    indices = np.random.randint(0, len(noise_pool), size=len(data))\n",
    "    noise_samples = noise_pool[indices]\n",
    "    \n",
    "    return data + scale * noise_samples\n",
    "\n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_train = train_waveforms\n",
    "d_eq_train  = X_train[y_train == 0]\n",
    "d_exp_train = X_train[y_train == 1]\n",
    "d_noise_train = X_train[y_train == 2]\n",
    "d_su_train  = X_train[y_train == 3]\n",
    "\n",
    "\n",
    "\n",
    "aug_eq  = augment_with_noise_numpy(d_eq_train, d_noise_train, scale=1)\n",
    "aug_exp = augment_with_noise_numpy(d_exp_train, d_noise_train, scale=1)\n",
    "aug_su  = augment_with_noise_numpy(d_su_train,  d_noise_train, scale= 1)\n",
    "aug_no  = augment_with_noise_numpy(d_noise_train,  d_noise_train, scale= 1)\n",
    "\n",
    "\n",
    "X_train_aug = np.concatenate([d_eq_train, d_exp_train, d_su_train, d_noise_train,\n",
    "                         aug_eq, aug_exp, aug_su, aug_no], axis=0)\n",
    "y_train_aug = torch.tensor(\n",
    "    [0]*len(d_eq_train) + [1]*len(d_exp_train) + [3]*len(d_su_train) + [2]*len(d_noise_train) +\n",
    "    [0]*len(aug_eq)      + [1]*len(aug_exp)      + [3]*len(aug_su) +[2]*len(aug_no)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eafbea1",
   "metadata": {},
   "source": [
    "## Preparing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "578e631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 363, Val batches: 121\n",
      "Train batches: 363, Val batches: 121\n"
     ]
    }
   ],
   "source": [
    "X_1d = X_train_aug\n",
    "X_1d = torch.tensor(X_1d, dtype = torch.float32)\n",
    "\n",
    "spec = compute_spectrogram(X_1d, 50)\n",
    "norm_spec = normalize_spectrogram_minmax(spec[0])\n",
    "\n",
    "X_2d = norm_spec\n",
    "y = y_train_aug\n",
    "\n",
    "\n",
    "train_split =75\n",
    "val_split = 25\n",
    "\n",
    "train_loader_1d, val_loader_1d = return_train_val_loaders(X = X_1d, y = y, train_split = train_split, val_split = val_split, batch_size = batch_size)\n",
    "train_loader_2d, val_loader_2d = return_train_val_loaders(X = X_2d, y = y, train_split = train_split, val_split = val_split, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4323f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330126d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surface",
   "language": "python",
   "name": "surface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
