{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f83e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# === Standard Libraries ===\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "# === Scientific Libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# === Signal Processing ===\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, correlate\n",
    "\n",
    "# === Seismology Libraries ===\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# === Machine Learning Libraries ===\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === File Handling ===\n",
    "import h5py\n",
    "\n",
    "# === Custom Modules ===\n",
    "module_path = os.path.abspath(os.path.join('../scripts'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neural_network_architectures import (\n",
    "     QuakeXNet_1d, QuakeXNet_2d, SeismicCNN_1d, SeismicCNN_2d )\n",
    "\n",
    "\n",
    "# === Seismology Client ===\n",
    "client = Client('IRIS')\n",
    "\n",
    "from utils import extract_waveforms\n",
    "from utils import compute_spectrogram\n",
    "from utils import normalize_spectrogram_minmax\n",
    "from utils import return_train_val_loaders\n",
    "from utils import plot_confusion_matrix_and_cr\n",
    "from utils import train_model\n",
    "from utils import WaveformPreprocessor\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6456a90",
   "metadata": {},
   "source": [
    "## I downloaded additional surface event data, the step below is just processing those additionally downloaded waveforms and these will be added later to the the total dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ba5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nos = 2000\n",
    "\n",
    "# if we are taking all data or not. \n",
    "all_data = False\n",
    "\n",
    "# the start point will be selected randomly from (start, -4)\n",
    "start = - 40\n",
    "shifting = True\n",
    "\n",
    "# training parameters\n",
    "train_split = 70                                      \n",
    "val_split=20\n",
    "test_split = 10\n",
    "learning_rate=0.001\n",
    "batch_size=128\n",
    "n_epochs=60\n",
    "dropout=0.4\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_channels = 3\n",
    "# new sampling rate\n",
    "fs = 50\n",
    "\n",
    "## filtering parameters\n",
    "highcut = 20\n",
    "lowcut = 1\n",
    "input_window_length = 100\n",
    "\n",
    "# randomly starting between -40 to -5s\n",
    "start = -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cf08fc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|██████████| 6495/6495 [00:13<00:00, 480.40it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_surface_events(data_path, ids_path, fs=50, original_fs = 100, lowcut=1, highcut=20, window_length=100, taper_alpha=0.1, random_offset=(-40, -5)):\n",
    "    \"\"\"\n",
    "    Processes surface event data by applying tapering, bandpass filtering, resampling, and normalization.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the surface event data (.npy file).\n",
    "        ids_path (str): Path to the surface event IDs (JSON file).\n",
    "        fs (int): Sampling rate for resampling.\n",
    "        lowcut (float): Low cutoff frequency for bandpass filter.\n",
    "        highcut (float): High cutoff frequency for bandpass filter.\n",
    "        window_length (int): Length of the waveform window (in seconds).\n",
    "        taper_alpha (float): Alpha value for the Tukey window.\n",
    "        random_offset (tuple): Range of random offsets for slicing data.\n",
    "        \n",
    "    Returns:\n",
    "        list: Processed waveform data.\n",
    "        list: Corresponding event IDs.\n",
    "    \"\"\"\n",
    "    # Load data and IDs\n",
    "    surface_data = np.load(data_path, allow_pickle=True)\n",
    "    with open(ids_path, \"r\") as file:\n",
    "        surface_ids = json.load(file)\n",
    "\n",
    "\n",
    "    processed_data = []\n",
    "    processed_ids = []\n",
    "\n",
    "    # Process each event\n",
    "    for i in tqdm(range(len(surface_data)), desc=\"Processing events\"):\n",
    "        try:\n",
    "            event_data = surface_data[i]\n",
    "            \n",
    "            orig_fs = 100\n",
    "            \n",
    "            # Randomly select a window of the specified length\n",
    "            random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "            \n",
    "            # assuming the onset at 90s\n",
    "            start_idx = int(90 * orig_fs) + random_shift\n",
    "            end_idx = start_idx + int(window_length * orig_fs)\n",
    "            \n",
    "    \n",
    "            # Handle boundary conditions\n",
    "            max_idx = event_data.shape[-1]\n",
    "            if end_idx > max_idx:\n",
    "                end_idx = max_idx\n",
    "                start_idx = end_idx - int(window_length * orig_fs)\n",
    "            if start_idx < 0:\n",
    "                start_idx = 0\n",
    "                end_idx = int(window_length * orig_fs)\n",
    "                \n",
    "                \n",
    "            sliced = event_data[:, start_idx:end_idx]\n",
    "            sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            processor = WaveformPreprocessor(\n",
    "            input_fs=original_fs,\n",
    "            target_fs=fs,\n",
    "            lowcut=lowcut,\n",
    "            highcut=highcut)\n",
    "            \n",
    "            processed = processor(sliced_tensor)  # (C, T)\n",
    "          \n",
    "        \n",
    "            if processed.shape[-1] != int(window_length*fs):\n",
    "                print('error')\n",
    "                continue\n",
    "\n",
    "            x = processed.numpy()\n",
    "            \n",
    "            if len(x) == 3:  # Ensure the event has three components\n",
    "                processed_data.append(x)\n",
    "                processed_ids.append(surface_ids[i])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log or print the exception if needed\n",
    "            print(f\"Error processing event {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return processed_data, processed_ids\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_path = '../../data/new_curated_surface_event_data.npy'\n",
    "ids_path = '../../data/new_curated_surface_event_ids.json'\n",
    "\n",
    "processed_additional_su, processed_id = process_surface_events(data_path, ids_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af80652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data files\n",
    "file_noise=\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\";\n",
    "file_comcat=  \"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\";\n",
    "file_exotic=\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\";\n",
    "\n",
    "# metadata\n",
    "# accessing the comcat metadata\n",
    "comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "# comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "\n",
    "# accessing the exotic metadata\n",
    "exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "# exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "\n",
    "# accessing the data files\n",
    "metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "# metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "# creating individual data frames for each class\n",
    "cat_exp = comcat_metadata[comcat_metadata['source_type'] == 'explosion']\n",
    "cat_eq = comcat_metadata[comcat_metadata['source_type'] == 'earthquake']\n",
    "cat_su = exotic_metadata[exotic_metadata['source_type'] == 'surface event']\n",
    "cat_noise = metadata_noise\n",
    "cat_noise['event_id'] = [cat_noise['trace_start_time'][i]+'_noise' for i in range(len(cat_noise))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ec9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the threshold\n",
    "SNR_THR = 1\n",
    "\n",
    "# explosions\n",
    "trace_snr_db_values = np.array([float(cat_exp.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_exp.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_exp = cat_exp.iloc[ii2]\n",
    "\n",
    "# earthquake\n",
    "trace_snr_db_values = np.array([float(cat_eq.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_eq.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_eq = cat_eq.iloc[ii2]\n",
    "\n",
    "# surface events\n",
    "trace_snr_db_values = np.array([float(cat_su.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_su.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR-2)[0].astype(int) \n",
    "df_su = cat_su.iloc[ii2]\n",
    "\n",
    "# noise\n",
    "# does not change\n",
    "df_noise = cat_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e9c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8434/8434 [01:28<00:00, 95.41it/s] \n",
      "  0%|          | 9/15000 [00:00<02:57, 84.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3778, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:59<00:00, 83.39it/s]\n",
      "  0%|          | 4/13638 [00:00<05:55, 38.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10529, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13638/13638 [03:35<00:00, 63.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8779, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [04:30<00:00, 62.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10477, 3, 5000)\n"
     ]
    }
   ],
   "source": [
    "number_data_per_class = len(df_su)\n",
    "# surface events\n",
    "d_su, id_su = extract_waveforms(df_su, file_exotic, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_su.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = 15000\n",
    "# noise\n",
    "d_noise, id_noise = extract_waveforms(df_noise, file_noise, input_window_length = input_window_length, fs=fs,\n",
    "                                      start = start, number_data = number_data_per_class,\n",
    "                                      num_channels = num_channels, shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_noise.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = len(df_exp)\n",
    "# explosions\n",
    "d_exp, id_exp = extract_waveforms(df_exp, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                  start = start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                  shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "\n",
    "print(d_exp.shape)\n",
    "\n",
    "\n",
    "number_data_per_class = 17000\n",
    "# earthquakes\n",
    "d_eq, id_eq = extract_waveforms(df_eq, file_comcat, input_window_length = input_window_length,  fs=fs,\n",
    "                                start =start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_eq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5cbc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d48d642",
   "metadata": {},
   "source": [
    "## Preparing the esec waveforms in standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43455e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/curated_esec_catalog_for_retraining.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87ebc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id</th>\n",
       "      <th>station</th>\n",
       "      <th>prob_eq</th>\n",
       "      <th>prob_px</th>\n",
       "      <th>prob_noise</th>\n",
       "      <th>prob_surface</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>assigned_label</th>\n",
       "      <th>snr_n</th>\n",
       "      <th>event_label_x</th>\n",
       "      <th>event_label_y</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74</td>\n",
       "      <td>SLD</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>0.606736</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>0.979550</td>\n",
       "      <td>0.979550</td>\n",
       "      <td>surface</td>\n",
       "      <td>10.354430</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>rock avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130</td>\n",
       "      <td>PLBC</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.965852</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.022586</td>\n",
       "      <td>0.965852</td>\n",
       "      <td>px</td>\n",
       "      <td>12.442687</td>\n",
       "      <td>px</td>\n",
       "      <td>px</td>\n",
       "      <td>rock avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160</td>\n",
       "      <td>N19K</td>\n",
       "      <td>0.077101</td>\n",
       "      <td>0.220457</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.766895</td>\n",
       "      <td>0.766895</td>\n",
       "      <td>surface</td>\n",
       "      <td>12.884728</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>ice avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160</td>\n",
       "      <td>O18K</td>\n",
       "      <td>0.241163</td>\n",
       "      <td>0.072579</td>\n",
       "      <td>0.998638</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>0.925733</td>\n",
       "      <td>surface</td>\n",
       "      <td>13.624922</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>ice avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160</td>\n",
       "      <td>O19K</td>\n",
       "      <td>0.036180</td>\n",
       "      <td>0.535932</td>\n",
       "      <td>0.995490</td>\n",
       "      <td>0.585888</td>\n",
       "      <td>0.585888</td>\n",
       "      <td>surface</td>\n",
       "      <td>7.214448</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>ice avalanche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251</th>\n",
       "      <td>227</td>\n",
       "      <td>D156</td>\n",
       "      <td>0.565975</td>\n",
       "      <td>0.729836</td>\n",
       "      <td>0.999775</td>\n",
       "      <td>0.040341</td>\n",
       "      <td>0.729836</td>\n",
       "      <td>px</td>\n",
       "      <td>8.105497</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>rock fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>227</td>\n",
       "      <td>D160</td>\n",
       "      <td>0.705087</td>\n",
       "      <td>0.286322</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.959258</td>\n",
       "      <td>0.959258</td>\n",
       "      <td>surface</td>\n",
       "      <td>9.703775</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>rock fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>227</td>\n",
       "      <td>D163</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.028818</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.980757</td>\n",
       "      <td>0.980757</td>\n",
       "      <td>surface</td>\n",
       "      <td>22.175401</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>rock fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>227</td>\n",
       "      <td>KBA</td>\n",
       "      <td>0.015515</td>\n",
       "      <td>0.234649</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.906808</td>\n",
       "      <td>0.906808</td>\n",
       "      <td>surface</td>\n",
       "      <td>74.285297</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>rock fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>227</td>\n",
       "      <td>LESA</td>\n",
       "      <td>0.017449</td>\n",
       "      <td>0.991058</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.164146</td>\n",
       "      <td>0.991058</td>\n",
       "      <td>px</td>\n",
       "      <td>10.337358</td>\n",
       "      <td>surface</td>\n",
       "      <td>surface</td>\n",
       "      <td>rock fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1866 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      event_id station   prob_eq   prob_px  prob_noise  prob_surface  \\\n",
       "0           74     SLD  0.009431  0.606736    0.999952      0.979550   \n",
       "1          130    PLBC  0.002547  0.965852    0.999910      0.022586   \n",
       "2          160    N19K  0.077101  0.220457    0.999976      0.766895   \n",
       "3          160    O18K  0.241163  0.072579    0.998638      0.925733   \n",
       "4          160    O19K  0.036180  0.535932    0.995490      0.585888   \n",
       "...        ...     ...       ...       ...         ...           ...   \n",
       "2251       227    D156  0.565975  0.729836    0.999775      0.040341   \n",
       "2252       227    D160  0.705087  0.286322    0.999996      0.959258   \n",
       "2254       227    D163  0.000114  0.028818    0.999224      0.980757   \n",
       "2255       227     KBA  0.015515  0.234649    0.999980      0.906808   \n",
       "2256       227    LESA  0.017449  0.991058    0.999997      0.164146   \n",
       "\n",
       "      max_prob assigned_label      snr_n event_label_x event_label_y  \\\n",
       "0     0.979550        surface  10.354430       surface       surface   \n",
       "1     0.965852             px  12.442687            px            px   \n",
       "2     0.766895        surface  12.884728       surface       surface   \n",
       "3     0.925733        surface  13.624922       surface       surface   \n",
       "4     0.585888        surface   7.214448       surface       surface   \n",
       "...        ...            ...        ...           ...           ...   \n",
       "2251  0.729836             px   8.105497       surface       surface   \n",
       "2252  0.959258        surface   9.703775       surface       surface   \n",
       "2254  0.980757        surface  22.175401       surface       surface   \n",
       "2255  0.906808        surface  74.285297       surface       surface   \n",
       "2256  0.991058             px  10.337358       surface       surface   \n",
       "\n",
       "                type  \n",
       "0     rock avalanche  \n",
       "1     rock avalanche  \n",
       "2      ice avalanche  \n",
       "3      ice avalanche  \n",
       "4      ice avalanche  \n",
       "...              ...  \n",
       "2251       rock fall  \n",
       "2252       rock fall  \n",
       "2254       rock fall  \n",
       "2255       rock fall  \n",
       "2256       rock fall  \n",
       "\n",
       "[1866 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29fd7be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:31<00:00, 58.95it/s]\n"
     ]
    }
   ],
   "source": [
    "esec_data = []\n",
    "esec_ids = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        event_id = df['event_id'].iloc[i]\n",
    "        station = df['station'].iloc[i]\n",
    "\n",
    "        # Find all vertical component files for the event and station\n",
    "        files = glob(f\"../../data/iris_esec_waveforms/waveforms/{event_id}/*{station}*\")\n",
    "\n",
    "        if len(files) == 3:\n",
    "            st = obspy.Stream()\n",
    "            for file in files:\n",
    "                st += obspy.read(file)\n",
    "            \n",
    "            \n",
    "            st.resample(100)\n",
    "\n",
    "            # Convert to NumPy array and clip length to 27000 samples (if possible)\n",
    "            arr = np.stack([tr.data[:27000] for tr in st])\n",
    "            esec_data.append(arr)\n",
    "            esec_ids.append(event_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on index {i}, event {df['event_id'].iloc[i]}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0fcba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "esec_data = np.array(esec_data)\n",
    "esec_ids = np.array(esec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6df67427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:03<00:00, 489.53it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "processed_ids = []\n",
    "random_offset=(-40, -5)\n",
    "fs=50 \n",
    "original_fs = 100\n",
    "lowcut=1\n",
    "highcut=20\n",
    "window_length=100\n",
    "taper_alpha=0.1\n",
    "event_data = esec_data[i]\n",
    "orig_fs = 100\n",
    "\n",
    "processed_esec_data = []\n",
    "processed_esec_ids = []\n",
    "\n",
    "for i in tqdm(range(len(esec_data))):\n",
    "    event_data = esec_data[i]\n",
    "    random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "\n",
    "\n",
    "    # assuming the onset at 90s\n",
    "    start_idx = int(90 * orig_fs) + random_shift\n",
    "    end_idx = start_idx + int(window_length * orig_fs)\n",
    "\n",
    "    # Handle boundary conditions\n",
    "    max_idx = event_data.shape[-1]\n",
    "    if end_idx > max_idx:\n",
    "        end_idx = max_idx\n",
    "        start_idx = end_idx - int(window_length * orig_fs)\n",
    "    if start_idx < 0:\n",
    "        start_idx = 0\n",
    "        end_idx = int(window_length * orig_fs)\n",
    "\n",
    "\n",
    "\n",
    "    sliced = event_data[:, start_idx:end_idx]\n",
    "    sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    processor = WaveformPreprocessor(\n",
    "    input_fs=original_fs,\n",
    "    target_fs=fs,\n",
    "    lowcut=lowcut,\n",
    "    highcut=highcut)\n",
    "\n",
    "    processed = processor(sliced_tensor)  # (C, T)\n",
    "\n",
    "\n",
    "    if processed.shape[-1] != int(window_length*fs):\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "    x = processed.numpy()\n",
    "    \n",
    "    if len(x) == 3:  # Ensure the event has three components\n",
    "        processed_esec_data.append(x)\n",
    "        processed_esec_ids.append(esec_ids[i])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fe85c",
   "metadata": {},
   "source": [
    "## Ok so now we have a processed esec data, we will add it to the training data to augment model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634130b",
   "metadata": {},
   "source": [
    "## Preparing training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aa9f3318",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 78\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_waveforms, test_waveforms, y_train, y_test\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m new_X_1d, new_X, new_y \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_eq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_su\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m train_waveforms, test_waveforms, y_train, y_test \u001b[38;5;241m=\u001b[39m split_and_save_data(new_X, new_X_1d, new_y)\n",
      "Cell \u001b[0;32mIn[124], line 18\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(d_eq, d_exp, d_noise, d_su, processed, processed_ids, num_samples)\u001b[0m\n\u001b[1;32m     16\u001b[0m new_d_exp \u001b[38;5;241m=\u001b[39m d_exp[:num_samples]\n\u001b[1;32m     17\u001b[0m new_d_no \u001b[38;5;241m=\u001b[39m d_noise[:num_samples]\n\u001b[0;32m---> 18\u001b[0m new_d_su \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md_su\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:num_samples]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Extract corresponding event IDs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m new_id_eq \u001b[38;5;241m=\u001b[39m id_eq[:num_samples]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "def prepare_data(d_eq, d_exp, d_noise, d_su, processed_additional_su, processed_ids, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Prepares and concatenates seismic data for training and testing.\n",
    "    \n",
    "    Args:\n",
    "        d_eq, d_exp, d_noise, d_su (array): Arrays of waveform data for different classes.\n",
    "        processed (array): Processed surface waveforms.\n",
    "        processed_ids (array): Corresponding IDs for processed surface waveforms.\n",
    "        num_samples (int): Number of samples to extract per class.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined waveforms (new_X_1d), event IDs (new_X), and labels (new_y).\n",
    "    \"\"\"\n",
    "    # Extract the required number of samples per class\n",
    "    new_d_eq = d_eq[:num_samples]\n",
    "    new_d_exp = d_exp[:num_samples]\n",
    "    new_d_no = d_noise[:num_samples]\n",
    "    new_d_su = np.concatenate([d_su, np.array(processed)])[:num_samples]\n",
    "\n",
    "    # Extract corresponding event IDs\n",
    "    new_id_eq = id_eq[:num_samples]\n",
    "    new_id_exp = id_exp[:num_samples]\n",
    "    new_id_no = id_noise[:num_samples]\n",
    "    new_id_su = np.concatenate([id_su, np.array(processed_ids)])[:num_samples]\n",
    "\n",
    "    # Combine data into single arrays\n",
    "    new_X_1d = np.vstack([new_d_eq, new_d_exp, new_d_no, new_d_su])  # Waveforms\n",
    "    new_X = np.hstack([new_id_eq, new_id_exp, new_id_no, new_id_su])  # Event IDs\n",
    "    new_y = (\n",
    "        [0] * len(new_d_eq) + \n",
    "        [1] * len(new_d_exp) + \n",
    "        [2] * len(new_d_no) + \n",
    "        [3] * len(new_d_su)\n",
    "    )  # Labels\n",
    "    \n",
    "    return new_X_1d, new_X, new_y\n",
    "\n",
    "def split_and_save_data(new_X, new_X_1d, new_y, test_size=0.2, random_state=42, save_path=\"../../data\"):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing, and saves the results to disk.\n",
    "    \n",
    "    Args:\n",
    "        new_X (array): Event IDs.\n",
    "        new_X_1d (array): Waveform data.\n",
    "        new_y (list): Labels for the event IDs.\n",
    "        test_size (float): Proportion of test data.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        save_path (str): Directory to save the test data IDs.\n",
    "    \"\"\"\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        new_X, new_y, test_size=test_size, random_state=random_state, stratify=new_y\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of event IDs to indices\n",
    "    event_id_to_index = {event_id: idx for idx, event_id in enumerate(new_X)}\n",
    "    \n",
    "    # Retrieve indices for test and train data\n",
    "    test_indices = [event_id_to_index[event_id] for event_id in X_test]\n",
    "    train_indices = [event_id_to_index[event_id] for event_id in X_train]\n",
    "    \n",
    "    # Extract waveforms for train and test sets\n",
    "    test_waveforms = new_X_1d[test_indices]\n",
    "    train_waveforms = new_X_1d[train_indices]\n",
    "    \n",
    "    # Save test data IDs for future use\n",
    "    np.save(f\"{save_path}/common_test_data_id.npy\", X_test)\n",
    "    np.save(f\"{save_path}/common_test_data_for_deep_learning.npy\", test_waveforms)\n",
    "    np.save(f\"{save_path}/common_test_data_labels_for_deep_learning.npy\", y_test)\n",
    "    \n",
    "    print(\"Shape of test_waveforms:\", test_waveforms.shape)\n",
    "    print(\"Shape of train_waveforms:\", train_waveforms.shape)\n",
    "\n",
    "    return train_waveforms, test_waveforms, y_train, y_test\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "new_X_1d, new_X, new_y = prepare_data(d_eq, d_exp, d_noise, d_su, processed, processed_id, num_samples=10000)\n",
    "train_waveforms, test_waveforms, y_train, y_test = split_and_save_data(new_X, new_X_1d, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7bc12497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5000])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ed4c8c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3778, 3, 5000)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_su.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12472b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surface",
   "language": "python",
   "name": "surface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
