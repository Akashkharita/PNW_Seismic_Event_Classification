{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33adee1",
   "metadata": {},
   "source": [
    "### Model training updates\n",
    "\n",
    "update 1 - retraining the model with the clean esec data (non flows, non rockfalls, non earthquakes). \n",
    "\n",
    "update 2 - June 20, 2025 - retraining the model with 2502 near field  (0-50 km) explosions waveforms that were classified as surface events and earthquakes. \n",
    "\n",
    "update 3 - retraining the model by assigning higher penalities whenever there is a confusion between explosion and surface events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e9ac8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# === Standard Libraries ===\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "# === Scientific Libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# === Signal Processing ===\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, correlate\n",
    "\n",
    "# === Seismology Libraries ===\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# === Machine Learning Libraries ===\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === File Handling ===\n",
    "import h5py\n",
    "\n",
    "# === Custom Modules ===\n",
    "module_path = os.path.abspath(os.path.join('../scripts'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neural_network_architectures import (\n",
    "     QuakeXNet_1d, QuakeXNet_2d, SeismicCNN_1d, SeismicCNN_2d )\n",
    "\n",
    "\n",
    "# === Seismology Client ===\n",
    "client = Client('IRIS')\n",
    "\n",
    "from utils import extract_waveforms\n",
    "from utils import compute_spectrogram\n",
    "from utils import normalize_spectrogram_minmax\n",
    "from utils import return_train_val_loaders\n",
    "from utils import plot_confusion_matrix_and_cr\n",
    "from utils import train_model\n",
    "from utils import train_model_weighted\n",
    "from utils import WaveformPreprocessor\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fee45f",
   "metadata": {},
   "source": [
    "## Defining some common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f61b3152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are taking all data or not. \n",
    "all_data = False\n",
    "\n",
    "# the start point will be selected randomly from (start, -4)\n",
    "start = - 40\n",
    "shifting = True\n",
    "\n",
    "# training parameters\n",
    "train_split = 70                                      \n",
    "val_split=20\n",
    "test_split = 10\n",
    "learning_rate=0.001\n",
    "batch_size=128\n",
    "n_epochs=60\n",
    "dropout=0.4\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_channels = 3\n",
    "# new sampling rate\n",
    "fs = 50\n",
    "\n",
    "## filtering parameters\n",
    "highcut = 20\n",
    "lowcut = 1\n",
    "input_window_length = 100\n",
    "\n",
    "# randomly starting between -40 to -5s\n",
    "start = -40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f677dab",
   "metadata": {},
   "source": [
    "## 1. Additional surface events per station. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49787d03",
   "metadata": {},
   "source": [
    "###  I downloaded additional surface event data by downloading from more stations from event to supplement existing data.  the step below is just processing those additionally downloaded waveforms and these will be added later to the the total dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d284597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|██████████| 6495/6495 [00:11<00:00, 588.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of additional surface event waveforms 6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_surface_events(data_path, ids_path, fs=50, original_fs = 100, lowcut=1, highcut=20, window_length=100, taper_alpha=0.1, random_offset=(-40, -5)):\n",
    "    \"\"\"\n",
    "    Processes surface event data by applying tapering, bandpass filtering, resampling, and normalization.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the surface event data (.npy file).\n",
    "        ids_path (str): Path to the surface event IDs (JSON file).\n",
    "        fs (int): Sampling rate for resampling.\n",
    "        lowcut (float): Low cutoff frequency for bandpass filter.\n",
    "        highcut (float): High cutoff frequency for bandpass filter.\n",
    "        window_length (int): Length of the waveform window (in seconds).\n",
    "        taper_alpha (float): Alpha value for the Tukey window.\n",
    "        random_offset (tuple): Range of random offsets for slicing data.\n",
    "        \n",
    "    Returns:\n",
    "        list: Processed waveform data.\n",
    "        list: Corresponding event IDs.\n",
    "    \"\"\"\n",
    "    # Load data and IDs\n",
    "    surface_data = np.load(data_path, allow_pickle=True)\n",
    "    with open(ids_path, \"r\") as file:\n",
    "        surface_ids = json.load(file)\n",
    "\n",
    "\n",
    "    processed_data = []\n",
    "    processed_ids = []\n",
    "\n",
    "    # Process each event\n",
    "    for i in tqdm(range(len(surface_data)), desc=\"Processing events\"):\n",
    "        try:\n",
    "            event_data = surface_data[i]\n",
    "            \n",
    "            orig_fs = 100\n",
    "            \n",
    "            # Randomly select a window of the specified length\n",
    "            random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "            \n",
    "            # assuming the onset at 90s\n",
    "            start_idx = int(90 * orig_fs) + random_shift\n",
    "            end_idx = start_idx + int(window_length * orig_fs)\n",
    "            \n",
    "    \n",
    "            # Handle boundary conditions\n",
    "            max_idx = event_data.shape[-1]\n",
    "            if end_idx > max_idx:\n",
    "                end_idx = max_idx\n",
    "                start_idx = end_idx - int(window_length * orig_fs)\n",
    "            if start_idx < 0:\n",
    "                start_idx = 0\n",
    "                end_idx = int(window_length * orig_fs)\n",
    "                \n",
    "                \n",
    "            sliced = event_data[:, start_idx:end_idx]\n",
    "            sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            processor = WaveformPreprocessor(\n",
    "            input_fs=original_fs,\n",
    "            target_fs=fs,\n",
    "            lowcut=lowcut,\n",
    "            highcut=highcut)\n",
    "            \n",
    "            processed = processor(sliced_tensor)  # (C, T)\n",
    "          \n",
    "        \n",
    "            if processed.shape[-1] != int(window_length*fs):\n",
    "                print('error')\n",
    "                continue\n",
    "\n",
    "            x = processed.numpy()\n",
    "            \n",
    "            if len(x) == 3:  # Ensure the event has three components\n",
    "                processed_data.append(x)\n",
    "                processed_ids.append(surface_ids[i])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log or print the exception if needed\n",
    "            print(f\"Error processing event {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return processed_data, processed_ids\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_path = '../../data/new_curated_surface_event_data.npy'\n",
    "ids_path = '../../data/new_curated_surface_event_ids.json'\n",
    "\n",
    "processed_additional_su, processed_additional_su_id = process_surface_events(data_path, ids_path)\n",
    "\n",
    "print(f'Length of additional surface event waveforms {len(processed_additional_su)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5aa46",
   "metadata": {},
   "source": [
    "## 2. Original PNW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c37e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data files\n",
    "file_noise=\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\";\n",
    "file_comcat=  \"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\";\n",
    "file_exotic=\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\";\n",
    "\n",
    "# metadata\n",
    "# accessing the comcat metadata\n",
    "comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "\n",
    "\n",
    "# accessing the exotic metadata\n",
    "exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "\n",
    "\n",
    "# accessing the data files\n",
    "metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "\n",
    "# creating individual data frames for each class\n",
    "cat_exp = comcat_metadata[comcat_metadata['source_type'] == 'explosion']\n",
    "cat_eq = comcat_metadata[comcat_metadata['source_type'] == 'earthquake']\n",
    "cat_su = exotic_metadata[exotic_metadata['source_type'] == 'surface event']\n",
    "cat_noise = metadata_noise\n",
    "cat_noise['event_id'] = [cat_noise['trace_start_time'][i]+'_noise' for i in range(len(cat_noise))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5ac6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the threshold\n",
    "SNR_THR = 1\n",
    "\n",
    "# explosions\n",
    "trace_snr_db_values = np.array([float(cat_exp.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_exp.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_exp = cat_exp.iloc[ii2]\n",
    "\n",
    "# earthquake\n",
    "trace_snr_db_values = np.array([float(cat_eq.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_eq.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_eq = cat_eq.iloc[ii2]\n",
    "\n",
    "# surface events\n",
    "trace_snr_db_values = np.array([float(cat_su.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_su.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR-2)[0].astype(int) \n",
    "df_su = cat_su.iloc[ii2]\n",
    "\n",
    "# noise\n",
    "# does not change\n",
    "df_noise = cat_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46cd0ba",
   "metadata": {},
   "source": [
    "## Note that we are only selecting three components from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35e955a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8434/8434 [01:16<00:00, 110.32it/s]\n",
      "  0%|          | 8/15000 [00:00<03:15, 76.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3778, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:34<00:00, 97.04it/s] \n",
      "  0%|          | 5/13638 [00:00<04:34, 49.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10583, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13638/13638 [03:10<00:00, 71.62it/s]\n",
      "  0%|          | 0/17000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8829, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [03:59<00:00, 70.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10506, 3, 5000)\n"
     ]
    }
   ],
   "source": [
    "number_data_per_class = len(df_su)\n",
    "# surface events\n",
    "d_su, id_su = extract_waveforms(df_su, file_exotic, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_su.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = 15000\n",
    "# noise\n",
    "d_noise, id_noise = extract_waveforms(df_noise, file_noise, input_window_length = input_window_length, fs=fs,\n",
    "                                      start = start, number_data = number_data_per_class,\n",
    "                                      num_channels = num_channels, shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_noise.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = len(df_exp)\n",
    "# explosions\n",
    "d_exp, id_exp = extract_waveforms(df_exp, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                  start = start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                  shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "\n",
    "print(d_exp.shape)\n",
    "\n",
    "\n",
    "number_data_per_class = 17000\n",
    "# earthquakes\n",
    "d_eq, id_eq = extract_waveforms(df_eq, file_comcat, input_window_length = input_window_length,  fs=fs,\n",
    "                                start =start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_eq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0552b6a",
   "metadata": {},
   "source": [
    "## 3. ESEC waveforms (1866 waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37464dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/curated_esec_catalog_for_retraining.csv',index_col = 0)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d265f4",
   "metadata": {},
   "source": [
    "### In the following cell, we are loading 270s of the three component esec waveforms, (70s before and 200s after), and resampling them to 100 Hz as required for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1714cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:25<00:00, 74.13it/s]\n"
     ]
    }
   ],
   "source": [
    "esec_data = []\n",
    "esec_ids = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        event_id = df['event_id'].iloc[i]\n",
    "        station = df['station'].iloc[i]\n",
    "\n",
    "        # Find all vertical component files for the event and station\n",
    "        files = glob(f\"../../data/iris_esec_waveforms/waveforms/{event_id}/*{station}*\")\n",
    "\n",
    "        if len(files) == 3:\n",
    "            st = obspy.Stream()\n",
    "            for file in files:\n",
    "                st += obspy.read(file)\n",
    "            \n",
    "            \n",
    "            st.resample(100)\n",
    "\n",
    "            # Convert to NumPy array and clip length to 27000 samples (if possible)\n",
    "            arr = np.stack([tr.data[:27000] for tr in st])\n",
    "            esec_data.append(arr)\n",
    "            esec_ids.append(event_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on index {i}, event {df['event_id'].iloc[i]}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6dcbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "esec_data = np.array(esec_data)\n",
    "esec_ids = np.array(esec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f00ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:02<00:00, 624.46it/s]\n"
     ]
    }
   ],
   "source": [
    "random_offset=(-40, -5)\n",
    "fs=50 \n",
    "original_fs = 100\n",
    "lowcut=1\n",
    "highcut=20\n",
    "window_length=100\n",
    "taper_alpha=0.1\n",
    "orig_fs = 100\n",
    "\n",
    "processed_esec_data = []\n",
    "processed_esec_ids = []\n",
    "\n",
    "for i in tqdm(range(len(esec_data))):\n",
    "    event_data = esec_data[i]\n",
    "    random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "\n",
    "\n",
    "    # assuming the onset at 90s\n",
    "    start_idx = int(90 * orig_fs) + random_shift\n",
    "    end_idx = start_idx + int(window_length * orig_fs)\n",
    "\n",
    "    # Handle boundary conditions\n",
    "    max_idx = event_data.shape[-1]\n",
    "    if end_idx > max_idx:\n",
    "        end_idx = max_idx\n",
    "        start_idx = end_idx - int(window_length * orig_fs)\n",
    "    if start_idx < 0:\n",
    "        start_idx = 0\n",
    "        end_idx = int(window_length * orig_fs)\n",
    "\n",
    "\n",
    "\n",
    "    sliced = event_data[:, start_idx:end_idx]\n",
    "    sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    processor = WaveformPreprocessor(\n",
    "    input_fs=original_fs,\n",
    "    target_fs=fs,\n",
    "    lowcut=lowcut,\n",
    "    highcut=highcut)\n",
    "\n",
    "    processed = processor(sliced_tensor)  # (C, T)\n",
    "\n",
    "\n",
    "    if processed.shape[-1] != int(window_length*fs):\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "    x = processed.numpy()\n",
    "    \n",
    "    if len(x) == 3:  # Ensure the event has three components\n",
    "        processed_esec_data.append(x)\n",
    "        processed_esec_ids.append(esec_ids[i])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e557f",
   "metadata": {},
   "source": [
    "## 4. New near field explosion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d82a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp_df = pd.read_csv('../../data/curated_new_explosions_data_for_retraining.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a466dca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "print(len(new_exp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5505cb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2502/2502 [00:22<00:00, 112.55it/s]\n"
     ]
    }
   ],
   "source": [
    "new_exp_data = []\n",
    "new_exp_ids = []\n",
    "\n",
    "for i in tqdm(range(len(new_exp_df))):\n",
    "    try:\n",
    "        event_id = new_exp_df['event_id'].iloc[i]\n",
    "        station = new_exp_df['station'].iloc[i]\n",
    "\n",
    "        # Find all vertical component files for the event and station\n",
    "        files = glob(f\"../../data/pnw_new_explosion_2023_2025/waveforms/{event_id}/*{station}*\")\n",
    "\n",
    "        if len(files) == 3:\n",
    "            st = obspy.Stream()\n",
    "            for file in files:\n",
    "                st += obspy.read(file)\n",
    "            \n",
    "            \n",
    "            st.resample(100)\n",
    "\n",
    "            # Convert to NumPy array and clip length to 27000 samples (if possible)\n",
    "            arr = np.stack([tr.data[:27000] for tr in st])\n",
    "            new_exp_data.append(arr)\n",
    "            new_exp_ids.append(event_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on index {i}, event {new_exp_df['event_id'].iloc[i]}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2678ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp_data = np.array(new_exp_data)\n",
    "new_exp_ids = np.array(new_exp_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82601874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2502/2502 [00:04<00:00, 607.26it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_new_exp_data = []\n",
    "processed_new_exp_ids = []\n",
    "\n",
    "for i in tqdm(range(len(new_exp_data))):\n",
    "    event_data = new_exp_data[i]\n",
    "    random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "\n",
    "\n",
    "    # assuming the onset at 90s\n",
    "    start_idx = int(70 * orig_fs) + random_shift\n",
    "    end_idx = start_idx + int(window_length * orig_fs)\n",
    "\n",
    "    # Handle boundary conditions\n",
    "    max_idx = event_data.shape[-1]\n",
    "    if end_idx > max_idx:\n",
    "        end_idx = max_idx\n",
    "        start_idx = end_idx - int(window_length * orig_fs)\n",
    "    if start_idx < 0:\n",
    "        start_idx = 0\n",
    "        end_idx = int(window_length * orig_fs)\n",
    "\n",
    "\n",
    "\n",
    "    sliced = event_data[:, start_idx:end_idx]\n",
    "    sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    processor = WaveformPreprocessor(\n",
    "    input_fs=original_fs,\n",
    "    target_fs=fs,\n",
    "    lowcut=lowcut,\n",
    "    highcut=highcut)\n",
    "\n",
    "    processed = processor(sliced_tensor)  # (C, T)\n",
    "\n",
    "\n",
    "    if processed.shape[-1] != int(window_length*fs):\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "    x = processed.numpy()\n",
    "    \n",
    "    if len(x) == 3:  # Ensure the event has three components\n",
    "        processed_new_exp_data.append(x)\n",
    "        processed_new_exp_ids.append(new_exp_ids[i])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bdc9aa",
   "metadata": {},
   "source": [
    "## Preparing training and testing data\n",
    "\n",
    "## We are keeping it such that the original testing data remains unaffected. And all the new and additional esec and explosion events will be added to the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06b455c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_waveforms: (7766, 3, 5000)\n",
      "Shape of train_waveforms: (31063, 3, 5000)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(d_eq, d_exp, d_noise, d_su, processed = processed_additional_su, processed_ids = processed_additional_su_id, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Prepares and concatenates seismic data for training and testing.\n",
    "    \n",
    "    Args:\n",
    "        d_eq, d_exp, d_noise, d_su (array): Arrays of waveform data for different classes.\n",
    "        processed (array): Processed surface waveforms.\n",
    "        processed_ids (array): Corresponding IDs for processed surface waveforms.\n",
    "        num_samples (int): Number of samples to extract per class.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined waveforms (new_X_1d), event IDs (new_X), and labels (new_y).\n",
    "    \"\"\"\n",
    "    # Extract the required number of samples per class\n",
    "    new_d_eq = d_eq[:num_samples]\n",
    "    new_d_exp = d_exp[:num_samples]\n",
    "    new_d_no = d_noise[:num_samples]\n",
    "    new_d_su = np.concatenate([d_su, np.array(processed)])[:num_samples]\n",
    "\n",
    "    # Extract corresponding event IDs\n",
    "    new_id_eq = id_eq[:num_samples]\n",
    "    new_id_exp = id_exp[:num_samples]\n",
    "    new_id_no = id_noise[:num_samples]\n",
    "    new_id_su = np.concatenate([id_su, np.array(processed_ids)])[:num_samples]\n",
    "\n",
    "    # Combine data into single arrays\n",
    "    new_X_1d = np.vstack([new_d_eq, new_d_exp, new_d_no, new_d_su])  # Waveforms\n",
    "    new_X = np.hstack([new_id_eq, new_id_exp, new_id_no, new_id_su])  # Event IDs\n",
    "    new_y = (\n",
    "        [0] * len(new_d_eq) + \n",
    "        [1] * len(new_d_exp) + \n",
    "        [2] * len(new_d_no) + \n",
    "        [3] * len(new_d_su)\n",
    "    )  # Labels\n",
    "    \n",
    "    return new_X_1d, new_X, new_y\n",
    "\n",
    "def split_and_save_data(new_X, new_X_1d, new_y, test_size=0.2, random_state=42, save_path=\"../../data\"):\n",
    "    \"\"\"\n",
    "    Splits data into training and testing, and saves the results to disk.\n",
    "    \n",
    "    Args:\n",
    "        new_X (array): Event IDs.\n",
    "        new_X_1d (array): Waveform data.\n",
    "        new_y (list): Labels for the event IDs.\n",
    "        test_size (float): Proportion of test data.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        save_path (str): Directory to save the test data IDs.\n",
    "    \"\"\"\n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        new_X, new_y, test_size=test_size, random_state=random_state, stratify=new_y\n",
    "    )\n",
    "    \n",
    "    # Create a mapping of event IDs to indices\n",
    "    event_id_to_index = {event_id: idx for idx, event_id in enumerate(new_X)}\n",
    "    \n",
    "    # Retrieve indices for test and train data\n",
    "    test_indices = [event_id_to_index[event_id] for event_id in X_test]\n",
    "    train_indices = [event_id_to_index[event_id] for event_id in X_train]\n",
    "    \n",
    "    # Extract waveforms for train and test sets\n",
    "    test_waveforms = new_X_1d[test_indices]\n",
    "    train_waveforms = new_X_1d[train_indices]\n",
    "    \n",
    "    # Save test data IDs for future use\n",
    "    np.save(f\"{save_path}/common_test_data_id.npy\", X_test)\n",
    "    np.save(f\"{save_path}/common_test_data_for_deep_learning.npy\", test_waveforms)\n",
    "    np.save(f\"{save_path}/common_test_data_labels_for_deep_learning.npy\", y_test)\n",
    "    \n",
    "    print(\"Shape of test_waveforms:\", test_waveforms.shape)\n",
    "    print(\"Shape of train_waveforms:\", train_waveforms.shape)\n",
    "\n",
    "    return train_waveforms, test_waveforms, y_train, y_test\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "new_X_1d, new_X, new_y = prepare_data(d_eq, d_exp, d_noise, d_su, processed_additional_su, processed_additional_su_id, num_samples=10000)\n",
    "train_waveforms, test_waveforms, y_train, y_test = split_and_save_data(new_X, new_X_1d, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e3521",
   "metadata": {},
   "source": [
    "## Data Augmentation Strategy\n",
    "\n",
    "**Adding esec data to training dataset** - So we will first add the esec surface event waveforms to surface events.\n",
    "\n",
    "**Adding new explosion data** - we will add the new explosion waveforms to explosions and \n",
    "\n",
    "**Balancing of the class** - Then we will augment each class so that each class have equal number of waveforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "78ea8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_noise_numpy(data, noise_pool, scale = 1, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    indices = np.random.randint(0, len(noise_pool), size=len(data))\n",
    "    noise_samples = noise_pool[indices]\n",
    "    \n",
    "    return data + scale * noise_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "X_train = train_waveforms\n",
    "d_eq_train  = X_train[y_train == 0]\n",
    "d_exp_train = X_train[y_train == 1]\n",
    "d_noise_train = X_train[y_train == 2]\n",
    "d_su_train = X_train[y_train == 3]\n",
    "\n",
    "\n",
    "\n",
    "# augmented waveforms\n",
    "aug_eq  = augment_with_noise_numpy(d_eq_train, d_noise_train, scale=1)\n",
    "aug_exp = augment_with_noise_numpy(d_exp_train, d_noise_train, scale=1)\n",
    "aug_su  = augment_with_noise_numpy(d_su_train,  d_noise_train, scale= 1)\n",
    "aug_no  = augment_with_noise_numpy(d_noise_train,  d_noise_train, scale= 1)\n",
    "\n",
    "\n",
    "# adding the esec data to surface events\n",
    "d_su_train = np.concatenate([d_su_train, np.array(processed_esec_data)], axis = 0)\n",
    "\n",
    "# adding the near field explosion data to explosions\n",
    "d_exp_train = np.concatenate([d_exp_train, np.array(processed_new_exp_data)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693a932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e881a7f",
   "metadata": {},
   "source": [
    "## Advantages and Drawbacks of Noise augmentation\n",
    "\n",
    "| Benefit                              | Why It Helps                                                                                                  |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------------------------------------- |\n",
    "| **Improves generalization**          | Forces the model to learn **robust patterns**, not memorize specific waveforms.                               |\n",
    "| **Simulates real-world variability** | Seismic data often contains environmental or instrument noise — this prepares your model for real deployment. |\n",
    "| **Reduces overfitting**              | Especially useful when you have a small dataset. It acts as regularization.                                   |\n",
    "| **Balances SNR**                     | Introduces variability in signal quality, helping models perform better on lower-SNR examples.                |\n",
    "\n",
    "\n",
    "\n",
    "| Risk / Side Effect             | Description                                                                                                          | Mitigation                                                                                                      |\n",
    "| ------------------------------ | -------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n",
    "| **Signal distortion**          | Excessive noise can alter key features like onsets or codas, degrading class-distinguishing patterns.                | Use low-amplitude noise (e.g., 5–15% of signal std). Avoid adding noise to already low-SNR signals.             |\n",
    "| **Over-representation bias**   | Doubling the dataset with noisy copies doesn’t add new diversity — may bias learning toward repeated waveform types. | Combine with other augmentations like time shifts, phase jitter, or filtering.                                  |\n",
    "| **Increased class confusion**  | Noise may blur differences between classes that are already hard to distinguish (e.g., surface vs explosion).        | Apply lighter or class-specific noise. Consider using contrastive loss to reinforce distinctions.               |\n",
    "| **Artificial feature leakage** | Reusing the same noise patterns (or fixed seeds) can introduce spurious consistency across batches.                  | Generate fresh, randomized noise every time, ideally per trace. Avoid deterministic augmentation.               |\n",
    "| **Validation misalignment**    | If validation data is clean but training is noisy, the model may underperform on actual deployment conditions.       | Consider augmenting validation data slightly, or better: validate on both clean and noisy datasets.             |\n",
    "| **Loss of phase integrity**    | Random additive noise can distort waveform phase, which is critical in seismology.                                   | Use **phase-preserving noise** (e.g., controlled Gaussian, colored noise, or filtered noise in relevant bands). |\n",
    "\n",
    "\n",
    "\n",
    "So we will try variations of the model with various levels of noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4f276d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of eqs in training: 8000\n",
      "No. of pxs in training: 9565\n",
      "No. of surface events in training: 9866\n",
      "No. of noise in training: 8000\n"
     ]
    }
   ],
   "source": [
    "print(f'No. of eqs in training: {len(d_eq_train)}')\n",
    "print(f'No. of pxs in training: {len(d_exp_train)}')\n",
    "print(f'No. of surface events in training: {len(d_su_train)}')\n",
    "print(f'No. of noise in training: {len(d_noise_train)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "25a3ba86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array([16000, 16000, 16000, 16000]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ensuring the class balance. \n",
    "aug_eq = aug_eq\n",
    "aug_no = aug_no\n",
    "aug_exp = aug_exp[:6435]\n",
    "aug_su = aug_su[:6134]\n",
    "\n",
    "X_train_aug = np.concatenate([d_eq_train, d_exp_train, d_su_train, d_noise_train,\n",
    "                         aug_eq, aug_exp, aug_su, aug_no], axis=0)\n",
    "                              \n",
    "y_train_aug = torch.tensor(\n",
    "    [0]*len(d_eq_train) + [1]*len(d_exp_train) + [3]*len(d_su_train) + [2]*len(d_noise_train) +\n",
    "    [0]*len(aug_eq)      + [1]*len(aug_exp)      + [3]*len(aug_su) +[2]*len(aug_no)\n",
    ")\n",
    "\n",
    "\n",
    "np.unique(y_train_aug.numpy(), return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8af4a",
   "metadata": {},
   "source": [
    "## So we now have 16k events per class in training dataset, when splitting this in 75:25, this will correspond to 12k events per class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4fa69e",
   "metadata": {},
   "source": [
    "# Preparing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "46b6a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 375, Val batches: 125\n",
      "Train batches: 375, Val batches: 125\n"
     ]
    }
   ],
   "source": [
    "X_1d = X_train_aug\n",
    "X_1d = torch.tensor(X_1d, dtype = torch.float32)\n",
    "\n",
    "spec = compute_spectrogram(X_1d, 50)\n",
    "norm_spec = normalize_spectrogram_minmax(spec[0])\n",
    "\n",
    "X_2d = norm_spec\n",
    "y = y_train_aug\n",
    "\n",
    "\n",
    "train_split =75\n",
    "val_split = 25\n",
    "\n",
    "train_loader_1d, val_loader_1d = return_train_val_loaders(X = X_1d, y = y, train_split = train_split, val_split = val_split, batch_size = batch_size)\n",
    "train_loader_2d, val_loader_2d = return_train_val_loaders(X = X_2d, y = y, train_split = train_split, val_split = val_split, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4dc9f8ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 39.75 MiB is free. Process 1377764 has 276.00 MiB memory in use. Process 2119647 has 276.00 MiB memory in use. Process 3084447 has 21.75 GiB memory in use. Including non-PyTorch memory, this process has 1.34 GiB memory in use. Of the allocated memory 1007.46 MiB is allocated by PyTorch, and 46.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model_seismiccnn_1d \u001b[38;5;241m=\u001b[39m \u001b[43mSeismicCNN_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use 'cuda' if you have a GPU available\u001b[39;00m\n\u001b[1;32m      4\u001b[0m (loss_time_seismiccnn_1d, val_loss_time_seismiccnn_1d, val_accuracy_time_seismiccnn_1d, model_training_time_seismiccnn_1d) \u001b[38;5;241m=\u001b[39m train_model_weighted(model_seismiccnn_1d,\n\u001b[1;32m      5\u001b[0m     train_loader_1d,  \n\u001b[1;32m      6\u001b[0m     val_loader_1d,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, \n\u001b[1;32m     12\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../trained_models/best_model_new_augmented_esec_exp_weighted_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/surface/lib/python3.9/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/surface/lib/python3.9/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/surface/lib/python3.9/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/surface/lib/python3.9/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 39.75 MiB is free. Process 1377764 has 276.00 MiB memory in use. Process 2119647 has 276.00 MiB memory in use. Process 3084447 has 21.75 GiB memory in use. Including non-PyTorch memory, this process has 1.34 GiB memory in use. Of the allocated memory 1007.46 MiB is allocated by PyTorch, and 46.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "device = 'cuda'\n",
    "model_seismiccnn_1d = SeismicCNN_1d(num_classes=4, num_channels=num_channels,dropout_rate=dropout).to(device)  # Use 'cuda' if you have a GPU available\n",
    "(loss_time_seismiccnn_1d, val_loss_time_seismiccnn_1d, val_accuracy_time_seismiccnn_1d, model_training_time_seismiccnn_1d) = train_model_weighted(model_seismiccnn_1d,\n",
    "    train_loader_1d,  \n",
    "    val_loader_1d,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    criterion=criterion,\n",
    "    augmentation= False, \n",
    "    patience = 30, \n",
    "    model_path = '../trained_models/best_model_new_augmented_esec_exp_weighted_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39911d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surface",
   "language": "python",
   "name": "surface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
