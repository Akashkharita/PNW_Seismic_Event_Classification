{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ef5aa6",
   "metadata": {},
   "source": [
    "### Model training updates\n",
    "\n",
    "update 1 - retraining the model with the clean esec data (non flows, non rockfalls, non earthquakes). \n",
    "\n",
    "update 2 - June 20, 2025 - retraining the model with 2502 near field  (0-50 km) explosions waveforms that were classified as surface events and earthquakes. \n",
    "\n",
    "update 3 - retraining the model by assigning higher penalities whenever there is a confusion between explosion and surface events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821258c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# === Standard Libraries ===\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "# === Scientific Libraries ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "# === Signal Processing ===\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, correlate\n",
    "\n",
    "# === Seismology Libraries ===\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "# === Machine Learning Libraries ===\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# === PyTorch ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === File Handling ===\n",
    "import h5py\n",
    "\n",
    "# === Custom Modules ===\n",
    "module_path = os.path.abspath(os.path.join('../scripts'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neural_network_architectures import (\n",
    "     QuakeXNet_1d, QuakeXNet_2d, SeismicCNN_1d, SeismicCNN_2d )\n",
    "\n",
    "\n",
    "# === Seismology Client ===\n",
    "client = Client('IRIS')\n",
    "\n",
    "from utils import extract_waveforms\n",
    "from utils import compute_spectrogram\n",
    "from utils import normalize_spectrogram_minmax\n",
    "from utils import return_train_val_loaders\n",
    "from utils import plot_confusion_matrix_and_cr\n",
    "from utils import train_model\n",
    "from utils import WaveformPreprocessor\n",
    "\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31663e",
   "metadata": {},
   "source": [
    "## Defining some common parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ce5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we are taking all data or not. \n",
    "all_data = False\n",
    "\n",
    "# the start point will be selected randomly from (start, -4)\n",
    "start = - 40\n",
    "shifting = True\n",
    "\n",
    "# training parameters\n",
    "train_split = 70                                      \n",
    "val_split=20\n",
    "test_split = 10\n",
    "learning_rate=0.001\n",
    "batch_size=128\n",
    "n_epochs=60\n",
    "dropout=0.4\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_channels = 3\n",
    "# new sampling rate\n",
    "fs = 50\n",
    "\n",
    "## filtering parameters\n",
    "highcut = 20\n",
    "lowcut = 1\n",
    "input_window_length = 100\n",
    "\n",
    "# randomly starting between -40 to -5s\n",
    "start = -40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4e9cb",
   "metadata": {},
   "source": [
    "## 1. Additional surface events per station. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58414dde",
   "metadata": {},
   "source": [
    "###  I downloaded additional surface event data by downloading from more stations from event to supplement existing data.  the step below is just processing those additionally downloaded waveforms and these will be added later to the the total dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831f391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing events: 100%|██████████| 6495/6495 [00:11<00:00, 588.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of additional surface event waveforms 6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_surface_events(data_path, ids_path, fs=50, original_fs = 100, lowcut=1, highcut=20, window_length=100, taper_alpha=0.1, random_offset=(-40, -5)):\n",
    "    \"\"\"\n",
    "    Processes surface event data by applying tapering, bandpass filtering, resampling, and normalization.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the surface event data (.npy file).\n",
    "        ids_path (str): Path to the surface event IDs (JSON file).\n",
    "        fs (int): Sampling rate for resampling.\n",
    "        lowcut (float): Low cutoff frequency for bandpass filter.\n",
    "        highcut (float): High cutoff frequency for bandpass filter.\n",
    "        window_length (int): Length of the waveform window (in seconds).\n",
    "        taper_alpha (float): Alpha value for the Tukey window.\n",
    "        random_offset (tuple): Range of random offsets for slicing data.\n",
    "        \n",
    "    Returns:\n",
    "        list: Processed waveform data.\n",
    "        list: Corresponding event IDs.\n",
    "    \"\"\"\n",
    "    # Load data and IDs\n",
    "    surface_data = np.load(data_path, allow_pickle=True)\n",
    "    with open(ids_path, \"r\") as file:\n",
    "        surface_ids = json.load(file)\n",
    "\n",
    "\n",
    "    processed_data = []\n",
    "    processed_ids = []\n",
    "\n",
    "    # Process each event\n",
    "    for i in tqdm(range(len(surface_data)), desc=\"Processing events\"):\n",
    "        try:\n",
    "            event_data = surface_data[i]\n",
    "            \n",
    "            orig_fs = 100\n",
    "            \n",
    "            # Randomly select a window of the specified length\n",
    "            random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "            \n",
    "            # assuming the onset at 90s\n",
    "            start_idx = int(90 * orig_fs) + random_shift\n",
    "            end_idx = start_idx + int(window_length * orig_fs)\n",
    "            \n",
    "    \n",
    "            # Handle boundary conditions\n",
    "            max_idx = event_data.shape[-1]\n",
    "            if end_idx > max_idx:\n",
    "                end_idx = max_idx\n",
    "                start_idx = end_idx - int(window_length * orig_fs)\n",
    "            if start_idx < 0:\n",
    "                start_idx = 0\n",
    "                end_idx = int(window_length * orig_fs)\n",
    "                \n",
    "                \n",
    "            sliced = event_data[:, start_idx:end_idx]\n",
    "            sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "            \n",
    "            processor = WaveformPreprocessor(\n",
    "            input_fs=original_fs,\n",
    "            target_fs=fs,\n",
    "            lowcut=lowcut,\n",
    "            highcut=highcut)\n",
    "            \n",
    "            processed = processor(sliced_tensor)  # (C, T)\n",
    "          \n",
    "        \n",
    "            if processed.shape[-1] != int(window_length*fs):\n",
    "                print('error')\n",
    "                continue\n",
    "\n",
    "            x = processed.numpy()\n",
    "            \n",
    "            if len(x) == 3:  # Ensure the event has three components\n",
    "                processed_data.append(x)\n",
    "                processed_ids.append(surface_ids[i])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log or print the exception if needed\n",
    "            print(f\"Error processing event {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return processed_data, processed_ids\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_path = '../../data/new_curated_surface_event_data.npy'\n",
    "ids_path = '../../data/new_curated_surface_event_ids.json'\n",
    "\n",
    "processed_additional_su, processed_additional_su_id = process_surface_events(data_path, ids_path)\n",
    "\n",
    "print(f'Length of additional surface event waveforms {len(processed_additional_su)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad6f1f",
   "metadata": {},
   "source": [
    "## 2. Original PNW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd950b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data files\n",
    "file_noise=\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\";\n",
    "file_comcat=  \"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\";\n",
    "file_exotic=\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\";\n",
    "\n",
    "# metadata\n",
    "# accessing the comcat metadata\n",
    "comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "\n",
    "\n",
    "# accessing the exotic metadata\n",
    "exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "\n",
    "\n",
    "# accessing the data files\n",
    "metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "\n",
    "# creating individual data frames for each class\n",
    "cat_exp = comcat_metadata[comcat_metadata['source_type'] == 'explosion']\n",
    "cat_eq = comcat_metadata[comcat_metadata['source_type'] == 'earthquake']\n",
    "cat_su = exotic_metadata[exotic_metadata['source_type'] == 'surface event']\n",
    "cat_noise = metadata_noise\n",
    "cat_noise['event_id'] = [cat_noise['trace_start_time'][i]+'_noise' for i in range(len(cat_noise))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d389ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the threshold\n",
    "SNR_THR = 1\n",
    "\n",
    "# explosions\n",
    "trace_snr_db_values = np.array([float(cat_exp.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_exp.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_exp = cat_exp.iloc[ii2]\n",
    "\n",
    "# earthquake\n",
    "trace_snr_db_values = np.array([float(cat_eq.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_eq.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR)[0].astype(int) \n",
    "df_eq = cat_eq.iloc[ii2]\n",
    "\n",
    "# surface events\n",
    "trace_snr_db_values = np.array([float(cat_su.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_su.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>SNR_THR-2)[0].astype(int) \n",
    "df_su = cat_su.iloc[ii2]\n",
    "\n",
    "# noise\n",
    "# does not change\n",
    "df_noise = cat_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ee3f5",
   "metadata": {},
   "source": [
    "## Note that we are only selecting three components from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41705f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8434/8434 [01:16<00:00, 110.32it/s]\n",
      "  0%|          | 8/15000 [00:00<03:15, 76.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3778, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:34<00:00, 97.04it/s] \n",
      "  0%|          | 5/13638 [00:00<04:34, 49.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10583, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13638/13638 [03:10<00:00, 71.62it/s]\n",
      "  0%|          | 0/17000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8829, 3, 5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17000/17000 [03:59<00:00, 70.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10506, 3, 5000)\n"
     ]
    }
   ],
   "source": [
    "number_data_per_class = len(df_su)\n",
    "# surface events\n",
    "d_su, id_su = extract_waveforms(df_su, file_exotic, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_su.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = 15000\n",
    "# noise\n",
    "d_noise, id_noise = extract_waveforms(df_noise, file_noise, input_window_length = input_window_length, fs=fs,\n",
    "                                      start = start, number_data = number_data_per_class,\n",
    "                                      num_channels = num_channels, shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_noise.shape)\n",
    "\n",
    "\n",
    "\n",
    "number_data_per_class = len(df_exp)\n",
    "# explosions\n",
    "d_exp, id_exp = extract_waveforms(df_exp, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                  start = start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                  shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "\n",
    "print(d_exp.shape)\n",
    "\n",
    "\n",
    "number_data_per_class = 17000\n",
    "# earthquakes\n",
    "d_eq, id_eq = extract_waveforms(df_eq, file_comcat, input_window_length = input_window_length,  fs=fs,\n",
    "                                start =start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data, lowcut = lowcut , highcut =highcut)\n",
    "print(d_eq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1a797",
   "metadata": {},
   "source": [
    "## 3. ESEC waveforms (1866 waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae835146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/curated_esec_catalog_for_retraining.csv',index_col = 0)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c207642",
   "metadata": {},
   "source": [
    "### In the following cell, we are loading 270s of the three component esec waveforms, (70s before and 200s after), and resampling them to 100 Hz as required for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f6738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:25<00:00, 74.13it/s]\n"
     ]
    }
   ],
   "source": [
    "esec_data = []\n",
    "esec_ids = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    try:\n",
    "        event_id = df['event_id'].iloc[i]\n",
    "        station = df['station'].iloc[i]\n",
    "\n",
    "        # Find all vertical component files for the event and station\n",
    "        files = glob(f\"../../data/iris_esec_waveforms/waveforms/{event_id}/*{station}*\")\n",
    "\n",
    "        if len(files) == 3:\n",
    "            st = obspy.Stream()\n",
    "            for file in files:\n",
    "                st += obspy.read(file)\n",
    "            \n",
    "            \n",
    "            st.resample(100)\n",
    "\n",
    "            # Convert to NumPy array and clip length to 27000 samples (if possible)\n",
    "            arr = np.stack([tr.data[:27000] for tr in st])\n",
    "            esec_data.append(arr)\n",
    "            esec_ids.append(event_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on index {i}, event {df['event_id'].iloc[i]}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b7926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "esec_data = np.array(esec_data)\n",
    "esec_ids = np.array(esec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02e14267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1866/1866 [00:02<00:00, 624.46it/s]\n"
     ]
    }
   ],
   "source": [
    "random_offset=(-40, -5)\n",
    "fs=50 \n",
    "original_fs = 100\n",
    "lowcut=1\n",
    "highcut=20\n",
    "window_length=100\n",
    "taper_alpha=0.1\n",
    "orig_fs = 100\n",
    "\n",
    "processed_esec_data = []\n",
    "processed_esec_ids = []\n",
    "\n",
    "for i in tqdm(range(len(esec_data))):\n",
    "    event_data = esec_data[i]\n",
    "    random_shift = np.random.randint(random_offset[0], random_offset[1]) * orig_fs\n",
    "\n",
    "\n",
    "    # assuming the onset at 90s\n",
    "    start_idx = int(90 * orig_fs) + random_shift\n",
    "    end_idx = start_idx + int(window_length * orig_fs)\n",
    "\n",
    "    # Handle boundary conditions\n",
    "    max_idx = event_data.shape[-1]\n",
    "    if end_idx > max_idx:\n",
    "        end_idx = max_idx\n",
    "        start_idx = end_idx - int(window_length * orig_fs)\n",
    "    if start_idx < 0:\n",
    "        start_idx = 0\n",
    "        end_idx = int(window_length * orig_fs)\n",
    "\n",
    "\n",
    "\n",
    "    sliced = event_data[:, start_idx:end_idx]\n",
    "    sliced_tensor = torch.tensor(sliced, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    processor = WaveformPreprocessor(\n",
    "    input_fs=original_fs,\n",
    "    target_fs=fs,\n",
    "    lowcut=lowcut,\n",
    "    highcut=highcut)\n",
    "\n",
    "    processed = processor(sliced_tensor)  # (C, T)\n",
    "\n",
    "\n",
    "    if processed.shape[-1] != int(window_length*fs):\n",
    "        print('error')\n",
    "        continue\n",
    "\n",
    "    x = processed.numpy()\n",
    "    \n",
    "    if len(x) == 3:  # Ensure the event has three components\n",
    "        processed_esec_data.append(x)\n",
    "        processed_esec_ids.append(esec_ids[i])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2238e59",
   "metadata": {},
   "source": [
    "## 4. New near field explosion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc32f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surface",
   "language": "python",
   "name": "surface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
