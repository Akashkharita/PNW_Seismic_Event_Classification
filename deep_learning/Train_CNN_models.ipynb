{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0547958",
   "metadata": {},
   "source": [
    "# Train and evaluate CNN models\n",
    "\n",
    "\n",
    "\n",
    "Author: Akash Kharita\n",
    "\n",
    "Date: 02/28/2024\n",
    "\n",
    "Modified by Marine Denolle on 06/20/24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e949bd1",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d066669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import obspy\n",
    "# from tqdm import tqdm\n",
    "from glob import glob\n",
    "# import time\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy import stats,signal\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed8eb1",
   "metadata": {},
   "source": [
    "## Importing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f219ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from design_CNN_models import Archtime\n",
    "# from design_CNN_models import Archtime_do\n",
    "# from design_CNN_models import WaveDecompNet\n",
    "# from design_CNN_models import WaveDecompNet_do\n",
    "# from design_CNN_models import SeismicCNN_batch\n",
    "# from design_CNN_models import SeismicCNN_batch_do\n",
    "# from design_CNN_models import SeismicNet\n",
    "# from design_CNN_models import SeismicNet_do\n",
    "\n",
    "\n",
    "# from neural_network_processing_functions import extract_datasets\n",
    "# from neural_network_processing_functions import train_model\n",
    "# from neural_network_processing_functions import plot_train_val_loss\n",
    "# from neural_network_processing_functions import plot_accuracy\n",
    "# from neural_network_processing_functions import extract_datasets_for_test\n",
    "# from neural_network_processing_functions import train_model_for_test\n",
    "# from neural_network_processing_functions import test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79ff4f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d33682",
   "metadata": {},
   "source": [
    "### Waveform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b9fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data files\n",
    "file_noise=\"/data/whd01/yiyu_data/PNWML/noise_waveforms.hdf5\";\n",
    "file_comcat=  \"/data/whd01/yiyu_data/PNWML/comcat_waveforms.hdf5\";\n",
    "file_exotic=\"/data/whd01/yiyu_data/PNWML/exotic_waveforms.hdf5\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b008d",
   "metadata": {},
   "source": [
    "### Waveform Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59012f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "# accessing the comcat metadata\n",
    "comcat_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/comcat_metadata.csv\")\n",
    "\n",
    "# accessing the exotic metadata\n",
    "exotic_metadata = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/exotic_metadata.csv\")\n",
    "\n",
    "# accessing the data files\n",
    "metadata_noise = pd.read_csv(\"/data/whd01/yiyu_data/PNWML/noise_metadata.csv\")\n",
    "\n",
    "# creating individual data frames for each class\n",
    "cat_exp = comcat_metadata[comcat_metadata['source_type'] == 'explosion']\n",
    "cat_eq = comcat_metadata[comcat_metadata['source_type'] == 'earthquake']\n",
    "cat_su = exotic_metadata[exotic_metadata['source_type'] == 'surface event']\n",
    "cat_noise = metadata_noise\n",
    "cat_noise['event_id'] = [cat_noise['trace_start_time'][i]+'_noise' for i in range(len(cat_noise))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab8e20",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "482b90db",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=-30\n",
    "input_window_length=100 # in seconds\n",
    "fs=50 # target sampling rate\n",
    "\n",
    "number_data_per_class=100 # number of data samples per class\n",
    "num_channels=3  # number of components to check\n",
    "\n",
    "all_data=False\n",
    "shifting=True\n",
    "\n",
    "\n",
    "# training parameters\n",
    "train_split = 80\n",
    "val_split=10\n",
    "test_split = 10\n",
    "learning_rate=0.001\n",
    "n_epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa147a",
   "metadata": {},
   "source": [
    "## Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3000d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a very simple CNN\n",
    "        \n",
    "class SeismicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_channels = 3):\n",
    "        super(SeismicCNN, self).__init__()\n",
    "        # Define the layers of the CNN architecture\n",
    "        self.conv1 = nn.Conv1d(in_channels= num_channels, out_channels=8, kernel_size=9,stride=1,padding='same')\n",
    "#         self.conv2 = nn.Conv1d(in_channels= 8, out_channels=8, kernel_size=9,stride=2,padding=4)       \n",
    "#         self.conv3 = nn.Conv1d(in_channels= 8, out_channels=16, kernel_size=7,stride=1,padding='same')\n",
    "#         self.conv4 = nn.Conv1d(in_channels= 16, out_channels=16, kernel_size=7,stride=2,padding=3)        \n",
    "#         self.conv5 = nn.Conv1d(in_channels= 16, out_channels=32, kernel_size=5,stride=1,padding='same')\n",
    "#         self.conv6 = nn.Conv1d(in_channels= 32, out_channels=32, kernel_size=5,stride=2,padding=2)                \n",
    "#         self.conv7 = nn.Conv1d(in_channels= 32, out_channels=64, kernel_size=3,stride=1,padding='same')\n",
    "        \n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # batch-normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(8)#, dtype=torch.float64)\n",
    "#         self.bn2 = nn.BatchNorm1d(8)#, dtype=torch.float64)\n",
    "#         self.bn3 = nn.BatchNorm1d(16)#, dtype=torch.float64)\n",
    "#         self.bn4 = nn.BatchNorm1d(16)#, dtype=torch.float64)\n",
    "#         self.bn5 = nn.BatchNorm1d(32)#, dtype=torch.float64)\n",
    "#         self.bn6 = nn.BatchNorm1d(32)#, dtype=torch.float64)\n",
    "#         self.bn7 = nn.BatchNorm1d(64)#, dtype=torch.float64)\n",
    "        \n",
    "        self.fc1 = nn.Linear(5000, 128)  # Adjust input size based on your data\n",
    "#         self.fc1 = nn.Linear(4992, 128)  # Adjust input size based on your data\n",
    "        self.fc2 = nn.Linear(128,4)  # Adjust input size based on your data\n",
    "        self.fc1_bn = nn.BatchNorm1d(128)\n",
    "        self.fc2_bn = nn.BatchNorm1d(num_classes)\n",
    "        \n",
    "        \n",
    "        # Calculate the input size for the first fully connected layer\n",
    "        fc_input_size = self._get_conv_output_size(num_channels, 5000)\n",
    "        \n",
    "        # define dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x))) # feature extraction, output size of 8,5000\n",
    "#         x = self.pool1(F.relu(self.bn2(self.conv2(x)))) # feature extraction, output size of 8,1250 \n",
    "        # max pooling is done at this point, but after the 2 convolutions to avoid aliasing\n",
    "#         x = F.relu(self.bn3(self.conv3(x))) # feature extraction, output size of 16,1250\n",
    "#         x = self.pool1(F.relu(self.bn4(self.conv4(x)))) # feature extraction, output size of 16, 312\n",
    "#         x =  F.relu(self.bn5(self.conv5(x))) # feature extraction, output size of 32, 312\n",
    "#         x = self.pool1(F.relu(self.bn6(self.conv6(x)))) # feature extraction, output size of 32, 78\n",
    "#         x = F.relu(self.bn7(self.conv7(x))) # feature extraction, output size of 64, 78            \n",
    "        x = x.view(x.size(0), -1) # Flatten before fully connected layer, 4992 features!\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))  # classifier\n",
    "        x = self.fc2_bn(self.fc2(x)) # classifier\n",
    "        \n",
    "        # Apply softmax for probabilities\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0145f",
   "metadata": {},
   "source": [
    "### test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa34f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the input shape is (batch_size, num_channels, num_features)\n",
    "batch_size = 1  # You can adjust the batch size as needed\n",
    "num_channels = 3\n",
    "num_features = 5000\n",
    "\n",
    "# Create a random input tensor with the specified shape\n",
    "random_input = torch.randn(batch_size, num_channels, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3902f604",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SeismicCNN' object has no attribute '_get_conv_output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize your model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSeismicCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Use 'cuda' if you have a GPU available\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Move the random input to the same device as your model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m random_input \u001b[38;5;241m=\u001b[39m random_input\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Use 'cuda' if you have a GPU available\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m, in \u001b[0;36mSeismicCNN.__init__\u001b[0;34m(self, num_classes, num_channels)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2_bn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(num_classes)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate the input size for the first fully connected layer\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m fc_input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conv_output_size\u001b[49m(num_channels, \u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# define dropout\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pnw_class/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SeismicCNN' object has no attribute '_get_conv_output_size'"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "model = SeismicCNN(num_classes=4, num_channels=num_channels).to(device)  # Use 'cuda' if you have a GPU available\n",
    "\n",
    "# Move the random input to the same device as your model\n",
    "random_input = random_input.to(device).float()  # Use 'cuda' if you have a GPU available\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Pass the random input through the model\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    output = model(random_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0314799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SeismicCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc78fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some more comments here\n",
    "from torch.utils.data import Dataset\n",
    "class PNWDataSet(Dataset): # create custom dataset\n",
    "    def __init__(self, data,labels,num_classes): # initialize\n",
    "        self.data = data \n",
    "        self.labels = labels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_data = self.data[index]\n",
    "        sample_labels = self.labels[index]\n",
    "        \n",
    "        # Convert labels to one-hot encoded vectors\n",
    "        sample_labels = torch.nn.functional.one_hot(torch.tensor(sample_labels), num_classes=self.num_classes)\n",
    "        print(sample_labels)\n",
    "        \n",
    "        return torch.Tensor(sample_data), sample_labels.float()  # return data as a tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571104de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_waveforms(cat, file_name, start=-20, input_window_length=100, fs=50, number_data=1000, num_channels=3, all_data=False, shifting=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is a function defined to extract the waveforms from file of waveforms and a dataframe of metadata. \n",
    "    The functions will also filter and resample the data if the data sampling rate is different from the target sampling rate\n",
    "    The data is shuffled in order it is called from the file it was stored in.\n",
    "    The data is shuffled in time by allowing a shift in selecting the waveform window with some of the pre-P data.\n",
    "    The data us normalized to its max(abs) on either component.\n",
    "    \n",
    "    Inputs:\n",
    "    cat -  Catalog containing metadata of the events, so we can extract the data using the bucket information\n",
    "    file_name - path of the h5py file containing the data\n",
    "    start - origin or first arrival time\n",
    "    num_features - window length to extract\n",
    "    before - number of samples to take before the arrival time\n",
    "    after - number of samples to take after the arrival time.\n",
    "    num_samples - no. of events per class to extract\n",
    "    \n",
    "    input_window_length: desired window length in seconds\n",
    "    fs: desired sampling rate.\n",
    "    num_channels - no. of channels per event to extract, if set 1, will extract Z component, if set any other number, will extract - ZNE component. \n",
    "    all_samples - if true, will extract all the samples corresponding of a given class\n",
    "    shifting - if true, will extract windows randomly starting between P-5, P-20. The random numbers follow a gaussian distribution. \n",
    "    Outputs:\n",
    "    \n",
    "    \"\"\"   \n",
    "    cat = cat.sample(frac=1).reset_index(drop=True)\n",
    "    if all_data:number_data = len(cat) # how many data to include\n",
    "    # open the file\n",
    "    f = h5py.File(file_name, 'r')\n",
    "    x=np.zeros(shape=(number_data,3,int(fs*input_window_length)))\n",
    "    event_ids = cat['event_id'].values\n",
    "    if not all_data:event_ids=event_ids[:number_data]\n",
    "        \n",
    "    for index in range(number_data):\n",
    "        # read data\n",
    "        bucket, narray = cat.loc[index]['trace_name'].split('$')\n",
    "        xx, _, _ = iter([int(i) for i in narray.split(',:')])\n",
    "        data = f['/data/%s' % bucket][xx, :, : ] # get all of the data\n",
    "        if fs != cat.loc[index,'trace_sampling_rate_hz']: #resample the data\n",
    "            nyquist = 0.5 * cat.loc[index,'trace_sampling_rate_hz']\n",
    "            low = 0.05 / nyquist;  high = 20 / nyquist\n",
    "            b, a = signal.butter(4, [low, high], btype='band')\n",
    "\n",
    "            # Apply the taper+filter to the signal\n",
    "            taper = signal.windows.tukey(data.shape[-1],alpha=0.1)\n",
    "            data = np.array([np.multiply(taper,row) for row in data])\n",
    "            filtered_signal = np.array([signal.filtfilt(b, a, row) for row in data])\n",
    "\n",
    "            # resample\n",
    "            number_of_samples = int(filtered_signal.shape[1] * fs / cat.loc[index,'trace_sampling_rate_hz'])\n",
    "            data = np.array([signal.resample(row, number_of_samples) for row in filtered_signal])\n",
    "\n",
    "            \n",
    "        if event_ids[index].split(\"_\")[-1]!=\"noise\":\n",
    "            #random start between P-20 and P-5 (upper bound is exclusive in numpy.random.randint)        \n",
    "            ii = int(np.random.randint(start,-4)*fs)\n",
    "            \n",
    "            if np.isnan(cat.loc[index, 'trace_P_arrival_sample']):continue\n",
    "            \n",
    "            istart = int(cat.loc[index, 'trace_P_arrival_sample']*fs/cat.loc[index,'trace_sampling_rate_hz']) + ii # start around the P\n",
    "            iend  = istart + int(fs*input_window_length)\n",
    "            if iend>data.shape[-1]:\n",
    "                istart = istart - (iend-data.shape[-1])\n",
    "                iend = data.shape[-1]\n",
    "        else:\n",
    "            istart=0\n",
    "            iend=istart+int(fs*input_window_length)\n",
    "\n",
    "        \n",
    "        # normalize the data\n",
    "        mmax = np.max(np.abs(data[:,istart:iend]))\n",
    "        # store data in big index\n",
    "        x[index,:,:iend-istart] = data[:,istart:iend]/mmax\n",
    "        \n",
    "        if num_channels==1:\n",
    "            x2 = x[:,2,:]\n",
    "            del x\n",
    "            x = x2\n",
    "            \n",
    "    # remove rows with zeros if there are any\n",
    "    idx=np.where(np.mean(np.abs(x[:,2,0:10]),axis=-1)>0)[0]\n",
    "                     \n",
    "    \n",
    "    f.close()\n",
    "    return x[idx,:,:], event_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d19086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, val_loader,  n_epochs=100, batch_size=32,learning_rate=0.001,criterion=nn.CrossEntropyLoss()):\n",
    "#     \"\"\"\n",
    "#     Function to train and evaluate the defined model.\n",
    "\n",
    "#     Parameters:\n",
    "#         model (torch.nn.Module): The neural network model.\n",
    "#         train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "#         val_dataset (torch.utils.data.Dataset): Validation dataset.\n",
    "#         val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "#         optimizer (torch.optim.Optimizer): Optimizer for training the model.\n",
    "#         n_epochs (int): Number of training epochs.\n",
    "#         batch_size (int): Batch size for training.\n",
    "#         number_input (int): Number of points in the input data.\n",
    "#         num_channels (int): Number of channels in the input data.\n",
    "\n",
    "#     Returns:\n",
    "#         accuracy_list (list): List of accuracies computed from each epoch.\n",
    "#         train_loss_list (list): List of training losses from each epoch.\n",
    "#         val_loss_list (list): List of validation losses from each epoch.\n",
    "#         y_pred (list): List of predicted values.\n",
    "#         y_true (list): List of true values.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     N_test = len(val_dataset)\n",
    "\n",
    "#     # to store the accuracies computed from each epoch.\n",
    "# #     accuracy_list = []\n",
    "\n",
    "#     # # Save loss and error for plotting\n",
    "#     loss_time = np.zeros(n_epochs)\n",
    "#     accuracy_time = np.zeros(n_epochs)\n",
    "\n",
    "#     # to store the predicted values\n",
    "# #     y_pred = []\n",
    "# #     y_true = []\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         running_loss = 0\n",
    "#         for data in train_loader:\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             inputs = inputs.float()\n",
    "#             labels = labels.long()\n",
    "            \n",
    "            \n",
    "#             # Set the parameter gradients to zero\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             outputs = model(inputs)\n",
    "            \n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # computing the gradients\n",
    "#             loss.backward()\n",
    "\n",
    "#             # updating the parameters\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # updating the training loss list\n",
    "#         loss_time[epoch] = running_loss/len(trainloader)\n",
    "\n",
    "#              # We evaluate the model, so we do not need the gradient\n",
    "#         with torch.no_grad(): # Context-manager that disabled gradient calculation.\n",
    "#             # Loop on samples in test set\n",
    "#             for data in trainloader:\n",
    "#                 # Get the sample and modify the format for PyTorch\n",
    "#                 inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#                 inputs = inputs.float() \n",
    "#                 labels = labels.long()\n",
    "#                 # Use model for sample in the test set\n",
    "#                 outputs = model(inputs)\n",
    "#                 # Compare predicted label and true label\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == labels).sum().item()\n",
    "#         # Save error at the end of each epochs\n",
    "#         accuracy_time[epoch] = 100 * correct / total\n",
    "\n",
    "#     # Print intermediate results on screen\n",
    "#     if testloader is not None:\n",
    "#         print('[Epoch %d] loss: %.3f - accuracy: %.3f' %\n",
    "#           (epoch + 1, running_loss/len(trainloader), 100 * correct / total))\n",
    "#     else:\n",
    "#         print('[Epoch %d] loss: %.3f' %\n",
    "#           (epoch + 1, running_loss/len(trainloader)))\n",
    "\n",
    "#     return loss_time, accuracy_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbe0eb",
   "metadata": {},
   "source": [
    "## Select only high SNR data\n",
    "\n",
    "based on the SNR value in the Z component, then store the reduced panda dataframe for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explosions\n",
    "trace_snr_db_values = np.array([float(cat_exp.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_exp.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>18)[0].astype(int) \n",
    "df_exp = cat_exp.iloc[ii2]\n",
    "\n",
    "# earthquake\n",
    "trace_snr_db_values = np.array([float(cat_eq.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_eq.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>18)[0].astype(int) \n",
    "df_eq = cat_eq.iloc[ii2]\n",
    "\n",
    "# surface events\n",
    "trace_snr_db_values = np.array([float(cat_su.loc[idx, 'trace_snr_db'].split(\"|\")[-1]) for idx in cat_su.index.values.tolist()])\n",
    "ii2= np.where(trace_snr_db_values>18)[0].astype(int) \n",
    "df_su = cat_su.iloc[ii2]\n",
    "\n",
    "# noise\n",
    "# does not change\n",
    "df_noise = cat_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978df7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5599a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface events\n",
    "d_su, id_su = extract_waveforms(df_su, file_exotic, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = False)\n",
    "print(d_su.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b102a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise\n",
    "d_noise, id_noise = extract_waveforms(df_noise, file_noise, input_window_length = input_window_length, fs=fs,\n",
    "                                      start = start, number_data = number_data_per_class,\n",
    "                                      num_channels = num_channels, shifting = shifting, all_data = all_data)\n",
    "print(d_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explosions\n",
    "d_exp, id_exp = extract_waveforms(df_exp, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                  start = start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                  shifting = shifting, all_data = all_data)\n",
    "\n",
    "print(d_exp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e4c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthquakes\n",
    "d_eq, id_eq = extract_waveforms(df_eq, file_comcat, input_window_length = input_window_length,  fs=fs,\n",
    "                                start =start,  number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = shifting, all_data = all_data)\n",
    "print(d_eq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2444e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all data into one input\n",
    "X = np.vstack([d_noise, d_exp, d_eq, d_su])\n",
    "print(X.shape)\n",
    "plt.plot(X[21,:,:].T)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ef07b",
   "metadata": {},
   "source": [
    "## Prepare labels\n",
    "labels to encode: here we understand that the classes are labeled as integers \n",
    "\n",
    "* 0: noise\n",
    "* 1: explosion\n",
    "* 2: earthquake\n",
    "* 3: surface event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# event_ids = ['noise']*len(d_noise)+['explosion']*len(d_exp)+['earthquake']*len(d_eq)+['surface']*len(d_su)\n",
    "event_ids = [0]*len(d_noise)+[1]*len(d_exp)+[2]*len(d_eq)+[3]*len(d_su)\n",
    "# y = np.hstack([id_noise, id_exp, id_eq, id_su])\n",
    "y = event_ids\n",
    "print(y)\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca452e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = PNWDataSet(X,y,4)\n",
    "for data,yy in custom_dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc78577",
   "metadata": {},
   "source": [
    "## Shuffle and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data a PNWDataSet\n",
    "custom_dataset = PNWDataSet(X,y,4)\n",
    "# first split train+val\n",
    "# Determine the size of the training set\n",
    "train_size = 317 #int(train_split/100 * len(custom_dataset)) # 80% of the data set\n",
    "val_size = 39#int(val_split/100 * len(custom_dataset)) # 10% of the data set\n",
    "test_size = 41#len(custom_dataset) - train_size - test_size # the rest is test\n",
    "print(train_size,val_size,test_size)\n",
    "print(len(custom_dataset))\n",
    "print([train_size, test_size+val_size])\n",
    "print(train_size+test_size+val_size)\n",
    "train_dataset, val_dataset = random_split(custom_dataset, [train_size, test_size+val_size])\n",
    "# then split val into val+test\n",
    "test_dataset, val_dataset = random_split(val_dataset, [test_size,val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "val_loader = DataLoader(val_dataset)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50031aa0",
   "metadata": {},
   "source": [
    "## Defining some common parameters for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13429d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function (e.g., Cross-Entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f460ef4",
   "metadata": {},
   "source": [
    "## Training and Testing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d598d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_dataset\n",
    "val_loader = val_dataset\n",
    "# def train_model(model, train_loader, val_loader,  n_epochs=100, batch_size=32,learning_rate=0.001,criterion=nn.CrossEntropyLoss()):\n",
    "# \"\"\"\n",
    "# Function to train and evaluate the defined model.\n",
    "\n",
    "# Parameters:\n",
    "#     model (torch.nn.Module): The neural network model.\n",
    "#     train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "#     val_dataset (torch.utils.data.Dataset): Validation dataset.\n",
    "#     val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "#     optimizer (torch.optim.Optimizer): Optimizer for training the model.\n",
    "#     n_epochs (int): Number of training epochs.\n",
    "#     batch_size (int): Batch size for training.\n",
    "#     number_input (int): Number of points in the input data.\n",
    "#     num_channels (int): Number of channels in the input data.\n",
    "\n",
    "# Returns:\n",
    "#     accuracy_list (list): List of accuracies computed from each epoch.\n",
    "#     train_loss_list (list): List of training losses from each epoch.\n",
    "#     val_loss_list (list): List of validation losses from each epoch.\n",
    "#     y_pred (list): List of predicted values.\n",
    "#     y_true (list): List of true values.\n",
    "# \"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "N_test = len(val_dataset)\n",
    "\n",
    "# to store the accuracies computed from each epoch.\n",
    "#     accuracy_list = []\n",
    "\n",
    "# # Save loss and error for plotting\n",
    "loss_time = np.zeros(n_epochs)\n",
    "accuracy_time = np.zeros(n_epochs)\n",
    "\n",
    "# to store the predicted values\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#         print(data)\n",
    "        print(data[0],inputs)\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.long()\n",
    "\n",
    "        \n",
    "\n",
    "        # Set the parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        print(\"ready to estimate first\")\n",
    "        print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(loss)\n",
    "        # computing the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # updating the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # updating the training loss list\n",
    "    loss_time[epoch] = running_loss/len(trainloader)\n",
    "\n",
    "         # We evaluate the model, so we do not need the gradient\n",
    "    with torch.no_grad(): # Context-manager that disabled gradient calculation.\n",
    "        # Loop on samples in test set\n",
    "        for data in trainloader:\n",
    "            # Get the sample and modify the format for PyTorch\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.float() \n",
    "            labels = labels.long()\n",
    "            # Use model for sample in the test set\n",
    "            outputs = model(inputs)\n",
    "            # Compare predicted label and true label\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    # Save error at the end of each epochs\n",
    "    accuracy_time[epoch] = 100 * correct / total\n",
    "\n",
    "# Print intermediate results on screen\n",
    "if testloader is not None:\n",
    "    print('[Epoch %d] loss: %.3f - accuracy: %.3f' %\n",
    "      (epoch + 1, running_loss/len(trainloader), 100 * correct / total))\n",
    "else:\n",
    "    print('[Epoch %d] loss: %.3f' %\n",
    "      (epoch + 1, running_loss/len(trainloader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f720137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeismicCNN(num_classes=4, num_channels=3)\n",
    "dummy_input = torch.randn(1, 3, 5000)\n",
    "output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6095c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, accuracy) = train_model(model,train_dataset,val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(np.arange(1, 101), loss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Correct predictions', color=color)\n",
    "ax2.plot(np.arange(1, 101), accuracy, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edd15a",
   "metadata": {},
   "source": [
    "## Archtime (Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archtime_normal\n",
    "num_channels = 1\n",
    "number_features = 5000\n",
    "\n",
    "#train_dataset, train_loader, test_dataset, test_loader, val_dataset, val_loader = extract_datasets(num_channels = 1, num_samples = 5000)\n",
    "#train_dataset, train_loader, y_train, test_dataset, test_loader, y_test,  val_dataset, val_loader, y_val = extract_datasets(before = 1000, after = 40000, num_samples = 5000, batch_size = 32, num_channels = 1, train_size = 4000, test_size = 0, num_features = 5000, shifting = True)\n",
    "\n",
    "\n",
    "data_loader_train,data_loader_val,data_loader_test = prepare_datasets()\n",
    "\n",
    "\n",
    "# train_dataset, train_loader, y_train, test_dataset, test_loader, y_test,  val_dataset, val_loader, y_val, event_ids_normal = extract_datasets(before = 1000, after = 4000, num_samples = 5500, batch_size = 32, num_channels = 1, train_size = 5000, test_size = 1, num_features = 5000, shifting = True, all_samples = False)\n",
    "\n",
    "\n",
    "model_archtime = Archtime(num_channels = 3, num_input = 5000)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_archtime.parameters(), lr=0.001)\n",
    "accuracy_archtime, train_loss_archtime, val_loss_archtime, y_pred, y_true  = train_model(model_archtime, train_loader, val_dataset, val_loader, optimizer, n_epochs = number_epochs, num_channels = num_channels, num_features = 5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnw_class",
   "language": "python",
   "name": "pnw_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
